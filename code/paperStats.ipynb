{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order of operations\n",
    "\n",
    "1. Gadid specimen get in CLAMS2ABL based on genetic IDs provided by Sharon Wildes (ArcticEIS repo, specimenCorrections modelLen.py)\n",
    "2. Run MBA for 201701 and 201901\n",
    "3. Export the following tables:\n",
    "    - catchData\n",
    "        - mbaCatchResults (mbaCatchResults.sql)\n",
    "        - v_aeis_catch_summary_v2\n",
    "        - v_aeis_events\n",
    "        - v_aeis_specimen\n",
    "            - Build trawl selectivity tables:\n",
    "                - Add consistent lengths for all gadid specimen (Arctic EIS repo, specimenCorrections addCommonLength.py)\n",
    "                - Export Basket and sample tables from CLAMS2ABL:\n",
    "                    `select * from baskets where (ship = 175 and (survey = 201701 or survey = 201901)) or (ship = 174)`\n",
    "                    `select * from samples where (ship = 175 and (survey = 201701 or survey = 201901)) or (ship = 174)`\n",
    "                - Add expansion values to the table (Arctic EIS repo, trawlSelectivity buildSelectivityTable.ipynb)\n",
    "        - AIERP_EventData (ArcticEIS, EventExport.sql)\n",
    "            - Calculate volme filtered (aeisEvents EventData.ipynb)\n",
    "    - acousticData\n",
    "        - mbaIntegrationResults (mbaResultsExport.sql)\n",
    "        - mbaSpeciesProp (see SQL code below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 transit speed (m/s):  3.3434855897494025\n",
      "47206.29784499888\n",
      "2019 transit speed (m/s):  3.420634502352993\n",
      "49963.68210099984\n"
     ]
    }
   ],
   "source": [
    "# Transit speed\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "files = glob('../data/acousticData/2017_2019/EV/2017/Echoview/exports/5m/*(intervals).csv')\n",
    "a = pd.concat(map(lambda file: pd.read_csv(file), files))\n",
    "a['Datetime_S'] = pd.to_datetime(a['Date_S'].astype(str)+a['Time_S'])\n",
    "a['Datetime_E'] = pd.to_datetime(a['Date_E'].astype(str)+a['Time_E'])\n",
    "a['Duration'] = (a.Datetime_E- a.Datetime_S).dt.total_seconds()\n",
    "a['Dist'] = a['VL_end']-a['VL_start']\n",
    "a['Speed'] = a.Dist*1852/a.Duration\n",
    "print('2017 transit speed (m/s): ',a.Speed.mean())\n",
    "print(sum((a.Dist)*(30/2)))\n",
    "\n",
    "from glob import glob\n",
    "files = glob('../data/acousticData/2017_2019/EV/2019/Echoview/exports/5m/*(intervals).csv')\n",
    "a = pd.concat(map(lambda file: pd.read_csv(file), files))\n",
    "a['Datetime_S'] = pd.to_datetime(a['Date_S'].astype(str)+' '+a['Time_S'])\n",
    "a['Datetime_E'] = pd.to_datetime(a['Date_E'].astype(str)+' '+a['Time_E'])\n",
    "a['Duration'] = (a.Datetime_E- a.Datetime_S).dt.total_seconds()\n",
    "a['Dist'] = a['VL_end']-a['VL_start']\n",
    "a['Speed'] = a.Dist*1852/a.Duration\n",
    "print('2019 transit speed (m/s): ',a.Speed.mean())\n",
    "print(sum((a.Dist)*(19)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017, 38 @ .512:  25.117 -0.855\n",
      "2019, 38 @ .512:  25.16 -0.8\n",
      "0.02231264103257835\n",
      "2017, 38 @ 4.0:  25.32 -0.24\n",
      "2019, 38 @ 4.0:  25.367 -0.28\n",
      "0.0016105112976727565\n"
     ]
    }
   ],
   "source": [
    "# Calibration\n",
    "# From the E:\\ChukchiTimeSeries\\data\\acousticData\\2017_2019\\EV\\2019\\calibration\\Ocean Starr EK60 cals AFSC.xslx spreadhseet\n",
    "\n",
    "# 38 kHz 512\n",
    "# 2017 \n",
    "# Cal1: 25.23, -0.88\n",
    "# Cal2: 25.00, -0.83\n",
    "print('2017, 38 @ .512: ',round(10*np.log10(((10**(25.23/10))+(10**(25.00/10)))/2),3),round(10*np.log10(((10**(-.88/10))+(10**(-.83/10)))/2),3))\n",
    "\n",
    "# 2019\n",
    "# Cal1: 25.17, -0.84\n",
    "# Cal2: 25.15, -0.76\n",
    "print('2019, 38 @ .512: ',round(10*np.log10(((10**(25.17/10))+(10**(25.15/10)))/2),3),round(10*np.log10(((10**(-.84/10))+(10**(-.76/10)))/2),3))\n",
    "\n",
    "\n",
    "print(1-(10**((25.117-.855)/10))/(10**((25.16-.8)/10)))\n",
    "\n",
    "# 38 kHz 4\n",
    "# 2017 (only 1 cal) 25.32,-0.24\n",
    "print('2017, 38 @ 4.0: ',25.32,-0.24)\n",
    "\n",
    "# 2019\n",
    "# Cal1: 25.25, -0.28\n",
    "# Cal2: 25.48, -0.28\n",
    "print('2019, 38 @ 4.0: ',round(10*np.log10(((10**(25.25/10))+(10**(25.48/10)))/2),3),round(10*np.log10(((10**(-.28/10))+(10**(-.28/10)))/2),3))\n",
    "\n",
    "print(1-(10**((25.32 -0.24)/10))/(10**((25.367 -0.28)/10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017, 120 @ .512:  24.79 -0.46\n",
      "2019, 120 @ .512:  24.959 -0.45\n",
      "0.04037843288965892\n",
      "2017, 120 @ 1:  24.56 -0.39\n",
      "2019, 120 @ 1:  24.933 -0.355\n",
      "0.08966760032009014\n"
     ]
    }
   ],
   "source": [
    "# 120 kHz 512\n",
    "# 2017 \n",
    "# Cal1: 25.07, -0.41\n",
    "# Cal2: 24.49, -0.51\n",
    "print('2017, 120 @ .512: ',round(10*np.log10(((10**(25.07/10))+(10**(24.49/10)))/2),3),round(10*np.log10(((10**(-.41/10))+(10**(-.51/10)))/2),3))\n",
    "\n",
    "# 2019\n",
    "# Cal1: 25.39, -0.46\n",
    "# Cal2: 24.48, -0.44\n",
    "print('2019, 120 @ .512: ',round(10*np.log10(((10**(25.39/10))+(10**(24.48/10)))/2),3),round(10*np.log10(((10**(-.46/10))+(10**(-.44/10)))/2),3))\n",
    "\n",
    "print(1-(10**((24.79 -0.46)/10))/(10**((24.959 -0.45)/10)))\n",
    "\n",
    "# 120 kHz 1\n",
    "# 2017 (only 1 cal) 24.56, -0.39\n",
    "print('2017, 120 @ 1: ',24.56, -0.39)\n",
    "\n",
    "# 2019\n",
    "# Cal1: 25.36, -0.35\n",
    "# Cal2: 24.46, -0.36\n",
    "print('2019, 120 @ 1: ',round(10*np.log10(((10**(25.36/10))+(10**(24.46/10)))/2),3),round(10*np.log10(((10**(-.35/10))+(10**(-.36/10)))/2),3))\n",
    "\n",
    "print(1-(10**((24.56 -0.39)/10))/(10**((24.933 -0.355)/10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 vert opening +STD:  8.280166666666666 1.12605543084403\n",
      "2017 hori opening +STD:  7.559833333333334 0.9346034410133652\n",
      "2019 vert opening +STD:  7.533414634146343 0.5061650410493478\n",
      "2019 hori opening +STD:  7.449062499999999 0.3429413509060847\n",
      "2017 HR mean, min, max : 27.12212121212121 11.46 46.78\n",
      "2019 HR mean, min, max : 34.98837209302326 13.27 227.9\n"
     ]
    }
   ],
   "source": [
    "# Net Openings\n",
    "b = pd.read_csv('../data/catchData/2017_2019/AIERP_EventData.csv')\n",
    "print('2017 vert opening +STD: ',b[b.SURVEY==201701].AVG_NET_VERT_OPENING.mean(),b[b.SURVEY==201701].AVG_NET_VERT_OPENING.std())\n",
    "print('2017 hori opening +STD: ',b[b.SURVEY==201701].AVG_NET_HORI_OPENING.mean(),b[b.SURVEY==201701].AVG_NET_HORI_OPENING.std())\n",
    "print('2019 vert opening +STD: ',b[b.SURVEY==201901].AVG_NET_VERT_OPENING.mean(),b[b.SURVEY==201901].AVG_NET_VERT_OPENING.std())\n",
    "print('2019 hori opening +STD: ',b[b.SURVEY==201901].AVG_NET_HORI_OPENING.mean(),b[b.SURVEY==201901].AVG_NET_HORI_OPENING.std())\n",
    "print('2017 HR mean, min, max :',b[b.SURVEY==201701].AVG_HEAD_ROPE_DEPTH.mean(),b[b.SURVEY==201701].AVG_HEAD_ROPE_DEPTH.min(),b[b.SURVEY==201701].AVG_HEAD_ROPE_DEPTH.max())\n",
    "print('2019 HR mean, min, max :',b[b.SURVEY==201901].AVG_HEAD_ROPE_DEPTH.mean(),b[b.SURVEY==201901].AVG_HEAD_ROPE_DEPTH.min(),b[b.SURVEY==201901].AVG_HEAD_ROPE_DEPTH.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# CTD per year:  68 55 39 46\n"
     ]
    }
   ],
   "source": [
    "# Number of CTD Stations\n",
    "c = pd.read_csv('catchAnalysis/analysisFiles/dfStat.csv')\n",
    "c = c[(~c.meanSa.isnull())&(~c.meanAcod.isnull())&(~c.meanPol.isnull())&(~c.meanCap.isnull())]\n",
    "print('# CTD per year: ',len(c[c.Year == 2012]),len(c[c.Year == 2013]),len(c[c.Year == 2017]),len(c[c.Year == 2019]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 All fishes  0.24789253188085802\n",
      "2017 All Arctic Cod  0.20975026751475034\n",
      "2019 All fishes  0.03478787966128983\n",
      "2019 All Arctic Cod  0.01788178450288881\n"
     ]
    }
   ],
   "source": [
    "# CPUE\n",
    "dfEvents = pd.read_csv('../data/catchData/2017_2019/AIERP_EventData.csv')\n",
    "catch = pd.read_csv('../data/catchData/2017_2019/catchExport.csv')\n",
    "events = pd.read_csv('../data/catchData/2017_2019/eventExport.csv')\n",
    "\n",
    "dfCatch = catch[(catch.GEAR == 'Marinovich') & (catch.SPECIES_CODE <40000) & (catch.SURVEY==201701)].merge(dfEvents, how='left',  left_on=['SURVEY','CLAMS_EVENT_NUMBER'],   right_on=['SURVEY','EVENT_ID'])\n",
    "dfCatch['CPUE'] = dfCatch.TOTAL_NUMBER_IN_HAUL/dfCatch.VOL_FILTERED\n",
    "dfCatch = dfCatch.groupby(by=['CLAMS_EVENT_NUMBER']).sum()\n",
    "print('2017 All fishes ',dfCatch.CPUE.mean())\n",
    "\n",
    "\n",
    "dfCatch = catch[(catch.GEAR == 'Marinovich') & (catch.SPECIES_CODE ==21725) & (catch.SURVEY==201701)].merge(dfEvents, how='left',  left_on=['SURVEY','CLAMS_EVENT_NUMBER'],   right_on=['SURVEY','EVENT_ID'])\n",
    "dfCatch['CPUE'] = dfCatch.TOTAL_NUMBER_IN_HAUL/dfCatch.VOL_FILTERED\n",
    "dfCatch = dfCatch.groupby(by=['CLAMS_EVENT_NUMBER']).sum()\n",
    "print('2017 All Arctic Cod ',dfCatch.CPUE.mean())\n",
    "\n",
    "dfCatch = catch[(catch.GEAR == 'Marinovich') & (catch.SPECIES_CODE <40000) & (catch.SURVEY==201901)].merge(dfEvents, how='left',  left_on=['SURVEY','CLAMS_EVENT_NUMBER'],   right_on=['SURVEY','EVENT_ID'])\n",
    "dfCatch['CPUE'] = dfCatch.TOTAL_NUMBER_IN_HAUL/dfCatch.VOL_FILTERED\n",
    "dfCatch = dfCatch.groupby(by=['CLAMS_EVENT_NUMBER']).sum()\n",
    "print('2019 All fishes ',dfCatch.CPUE.mean())\n",
    "\n",
    "\n",
    "dfCatch = catch[(catch.GEAR == 'Marinovich') & (catch.SPECIES_CODE ==21725) & (catch.SURVEY==201901)].merge(dfEvents, how='left',  left_on=['SURVEY','CLAMS_EVENT_NUMBER'],   right_on=['SURVEY','EVENT_ID'])\n",
    "dfCatch['CPUE'] = dfCatch.TOTAL_NUMBER_IN_HAUL/dfCatch.VOL_FILTERED\n",
    "dfCatch = dfCatch.groupby(by=['CLAMS_EVENT_NUMBER']).sum()\n",
    "print('2019 All Arctic Cod ',dfCatch.CPUE.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 length of 80 percentile:  7.3\n",
      "2019 length of 80 percentile:  8.0\n"
     ]
    }
   ],
   "source": [
    "# Lengths from catch\n",
    "a = pd.read_csv('../data/catchData/2017_2019/specimen_complete_selectivity.csv')\n",
    "import matplotlib.pyplot as plt\n",
    "b = a[(a.SURVEY==201701)&(~a.CONSISTENT_LENGTH.isnull())].CONSISTENT_LENGTH.values\n",
    "print('2017 length of 80 percentile: ',np.percentile(b,80))\n",
    "b = a[(a.SURVEY==201901)&(~a.CONSISTENT_LENGTH.isnull())].CONSISTENT_LENGTH.values\n",
    "print('2019 length of 80 percentile: ',np.percentile(b,80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017\n",
      "% fish abundance: 0.9863232129731636\n",
      "% Gad,cap,her abundance: 0.9627595542267834\n",
      "% fish weight: 0.16028538733154946\n",
      "% jelly abundance: 0.007698121572572092\n",
      "% jelly weight: 0.8383359547173879\n",
      "C. melanaster:  0.6088457142338402\n",
      "2019\n",
      "% fish abundance: 0.9342907275666883\n",
      "% Gad,cap,her abundance: 0.6369994922285663\n",
      "% fish weight: 0.06770644265539377\n",
      "% jelly abundance: 0.05539703561310407\n",
      "% jelly weight: 0.93123636563934\n",
      "C. melanaster:  0.5839014769756139\n"
     ]
    }
   ],
   "source": [
    "# Abundance and biomass from catch\n",
    "a = pd.read_csv('../data/catchData/2017_2019/catchExport.csv')\n",
    "a = a[(a.GEAR == 'Marinovich') & (a.NET_PARTITION == 'Codend')]\n",
    "a = a[a.SURVEY == 201701]\n",
    "a = a[(a.SPECIES_CODE >=1200)&(a.SPECIES_CODE < 60000)]\n",
    "aJel = np.unique(a[(a.SPECIES_CODE > 40000) & (a.SPECIES_CODE  < 50000)].SPECIES_CODE.values)# jellyfish\n",
    "aGad = np.unique( a[((a.SPECIES_CODE> 21700) & (a.SPECIES_CODE < 21750)) | (a.SPECIES_CODE == 1202)].SPECIES_CODE.values) # gadids    \n",
    "aSan =  np.unique(a[(a.SPECIES_CODE >= 20202) & (a.SPECIES_CODE <= 20204)].SPECIES_CODE.values) # sand lance\n",
    "aStic =  np.unique(a[(a.SPECIES_CODE >= 23800) & (a.SPECIES_CODE <= 23810)].SPECIES_CODE.values) # prickleback\n",
    "aCap =  np.unique(a[(a.SPECIES_CODE== 23041)].SPECIES_CODE.values) # capelin\n",
    "aHer =  np.unique(a[(a.SPECIES_CODE == 21110)].SPECIES_CODE.values) # herring\n",
    "print('2017')\n",
    "print('% fish abundance:', a[a.SPECIES_CODE.isin(np.concatenate([aGad,aSan,aStic,aCap,aHer]))].TOTAL_NUMBER_IN_HAUL.sum()/a.TOTAL_NUMBER_IN_HAUL.sum())\n",
    "print('% Gad,cap,her abundance:', a[a.SPECIES_CODE.isin(np.concatenate([aGad,aCap,aHer]))].TOTAL_NUMBER_IN_HAUL.sum()/a.TOTAL_NUMBER_IN_HAUL.sum())\n",
    "print('% fish weight:', a[a.SPECIES_CODE.isin(np.concatenate([aGad,aSan,aStic,aCap,aHer]))].TOTAL_WEIGHT_IN_HAUL.sum()/a.TOTAL_WEIGHT_IN_HAUL.sum())\n",
    "print('% jelly abundance:', a[a.SPECIES_CODE.isin(aJel)].TOTAL_NUMBER_IN_HAUL.sum()/a.TOTAL_NUMBER_IN_HAUL.sum())\n",
    "print('% jelly weight:', a[a.SPECIES_CODE.isin(aJel)].TOTAL_WEIGHT_IN_HAUL.sum()/a.TOTAL_WEIGHT_IN_HAUL.sum())\n",
    "print('C. melanaster: ',a[a.SPECIES_CODE==40504].TOTAL_WEIGHT_IN_HAUL.sum()/a[a.SPECIES_CODE.isin(aJel)].TOTAL_WEIGHT_IN_HAUL.sum())\n",
    "\n",
    "a = pd.read_csv('../data/catchData/2017_2019/catchExport.csv')\n",
    "a = a[(a.GEAR == 'Marinovich') & (a.NET_PARTITION == 'Codend')]\n",
    "a = a[a.SURVEY == 201901]\n",
    "a = a[(a.SPECIES_CODE >=1200)&(a.SPECIES_CODE < 60000)]\n",
    "aJel = np.unique(a[(a.SPECIES_CODE > 40000) & (a.SPECIES_CODE  < 50000)].SPECIES_CODE.values)# jellyfish\n",
    "aGad = np.unique( a[((a.SPECIES_CODE> 21700) & (a.SPECIES_CODE < 21750)) | (a.SPECIES_CODE == 1202)].SPECIES_CODE.values) # gadids    \n",
    "aSan =  np.unique(a[(a.SPECIES_CODE >= 20202) & (a.SPECIES_CODE <= 20204)].SPECIES_CODE.values) # sand lance\n",
    "aStic =  np.unique(a[(a.SPECIES_CODE >= 23800) & (a.SPECIES_CODE <= 23810)].SPECIES_CODE.values) # prickleback\n",
    "aCap =  np.unique(a[(a.SPECIES_CODE== 23041)].SPECIES_CODE.values) # capelin\n",
    "aHer =  np.unique(a[(a.SPECIES_CODE == 21110)].SPECIES_CODE.values) # herring\n",
    "print('2019')\n",
    "print('% fish abundance:', a[a.SPECIES_CODE.isin(np.concatenate([aGad,aSan,aStic,aCap,aHer]))].TOTAL_NUMBER_IN_HAUL.sum()/a.TOTAL_NUMBER_IN_HAUL.sum())\n",
    "print('% Gad,cap,her abundance:', a[a.SPECIES_CODE.isin(np.concatenate([aGad,aCap,aHer]))].TOTAL_NUMBER_IN_HAUL.sum()/a.TOTAL_NUMBER_IN_HAUL.sum())\n",
    "print('% fish weight:', a[a.SPECIES_CODE.isin(np.concatenate([aGad,aSan,aStic,aCap,aHer]))].TOTAL_WEIGHT_IN_HAUL.sum()/a.TOTAL_WEIGHT_IN_HAUL.sum())\n",
    "print('% jelly abundance:', a[a.SPECIES_CODE.isin(aJel)].TOTAL_NUMBER_IN_HAUL.sum()/a.TOTAL_NUMBER_IN_HAUL.sum())\n",
    "print('% jelly weight:', a[a.SPECIES_CODE.isin(aJel)].TOTAL_WEIGHT_IN_HAUL.sum()/a.TOTAL_WEIGHT_IN_HAUL.sum())\n",
    "print('C. melanaster: ',a[a.SPECIES_CODE==40504].TOTAL_WEIGHT_IN_HAUL.sum()/a[a.SPECIES_CODE.isin(aJel)].TOTAL_WEIGHT_IN_HAUL.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.90508847228669\n",
      "257.59256736688155\n",
      "1118.5119704976917\n",
      "178.53462277255952\n",
      "354.8068172398032\n",
      "2017 is X times higher than the other years:  15.556328233657856 4.343689320388349 6.266106442577031 3.152480270574972\n"
     ]
    }
   ],
   "source": [
    "# Backscatter\n",
    "df2012Summary = pd.read_csv('../data/acousticData/2012_2013/Arctic_EIS_Acoustic_trawl_survey_alongtrack_summary_2012_v3.csv')\n",
    "df2013Summary = pd.read_csv('../data/acousticData/2012_2013/Arctic_EIS_Acoustic_trawl_survey_alongtrack_summary_2013_v3.csv')\n",
    "df2017Summary = pd.read_csv('../data/acousticData/2017_2019/Arctic_EIS_Acoustic_trawl_survey_alongtrack_summary_2017.csv')\n",
    "df2018Summary = pd.read_csv('../data/acousticData/2018/Arctic_EIS_Acoustic_trawl_survey_alongtrack_summary_2018.csv')\n",
    "df2019Summary = pd.read_csv('../data/acousticData/2017_2019/Arctic_EIS_Acoustic_trawl_survey_alongtrack_summary_2019.csv')\n",
    "print(np.mean(df2012Summary['Fish 38 kHz sA (m^2 nmi^-2)'][(df2012Summary.Latitude > 66)]))# & (df2012Summary.Latitude  < 71.85)]))\n",
    "print(np.mean(df2013Summary['Fish 38 kHz sA (m^2 nmi^-2)'][(df2013Summary.Latitude > 66)]))# & (df2013Summary.Latitude  < 71.85)]))\n",
    "print(np.mean(df2017Summary['Fish 38 kHz sA (m^2 nmi^-2)'][(df2017Summary.Latitude > 66)]))# & (df2017Summary.Latitude  < 71.85)]))\n",
    "print(np.mean(df2018Summary['Fish 38 kHz sA (m^2 nmi^-2)'][(df2018Summary.Latitude > 66)]))# & (df2018Summary.Latitude  < 71.85)]))\n",
    "print(np.mean(df2019Summary['Fish 38 kHz sA (m^2 nmi^-2)'][(df2019Summary.Latitude > 66)]))# & (df2019Summary.Latitude  < 71.85)]))\n",
    "print('2017 is X times higher than the other years: ',1118.5/71.9,1118.5/257.5,1118.5/178.5,1118.5/354.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pollock 2017: 74757212076.9931\n",
      "Pollock 2019: 57404415222.94378\n",
      "Acod 2017: 898100743353.7224\n",
      "Acod 2019: 125555744574.5253\n",
      "Capelin 2017: 16160487002.227179\n",
      "Capelin 2019: 3739842338.6177306\n"
     ]
    }
   ],
   "source": [
    "a = pd.read_csv('../data/catchData/2017_2019/mbaCatchResults.csv')\n",
    "a.SPECIES_CODE.unique()\n",
    "#pollock\n",
    "print('Pollock 2017:',a[(a.SURVEY==201701)&(a.SPECIES_CODE==21744)].NUMBERS.sum())\n",
    "print('Pollock 2019:',a[(a.SURVEY==201901)&(a.SPECIES_CODE==21744)].NUMBERS.sum())\n",
    "#arctic cod\n",
    "print('Acod 2017:',a[(a.SURVEY==201701)&(a.SPECIES_CODE==21725)].NUMBERS.sum())\n",
    "print('Acod 2019:',a[(a.SURVEY==201901)&(a.SPECIES_CODE==21725)].NUMBERS.sum())\n",
    "# capelin\n",
    "print('Capelin 2017:',a[(a.SURVEY==201701)&(a.SPECIES_CODE==23041)].NUMBERS.sum())\n",
    "print('Capelin 2019:',a[(a.SURVEY==201901)&(a.SPECIES_CODE==23041)].NUMBERS.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 Gadids: 0.9838883646443122\n",
      "2019 Gadids: 0.9641131437639759\n",
      "2017 Arctic cod: 0.8953858684823652\n",
      "2019 Arctic cod: 0.6516882495320803\n",
      "2017 Pollock: 0.0745312279899934\n",
      "2019 Pollock: 0.29801677432207296\n"
     ]
    }
   ],
   "source": [
    "specs = [21725, 21744, 21740, 21735, 21720]\n",
    "print('2017 Gadids:',a[(a.SURVEY==201701)&(a.SPECIES_CODE.isin(specs))].NUMBERS.sum()/a[(a.SURVEY==201701)].NUMBERS.sum())\n",
    "print('2019 Gadids:',a[(a.SURVEY==201901)&(a.SPECIES_CODE.isin(specs))].NUMBERS.sum()/a[(a.SURVEY==201901)].NUMBERS.sum())\n",
    "\n",
    "print('2017 Arctic cod:',a[(a.SURVEY==201701)&(a.SPECIES_CODE==21725)].NUMBERS.sum()/a[(a.SURVEY==201701)].NUMBERS.sum())\n",
    "print('2019 Arctic cod:',a[(a.SURVEY==201901)&(a.SPECIES_CODE==21725)].NUMBERS.sum()/a[(a.SURVEY==201901)].NUMBERS.sum())\n",
    "\n",
    "print('2017 Pollock:',a[(a.SURVEY==201701)&(a.SPECIES_CODE.isin([21740,21744]))].NUMBERS.sum()/a[(a.SURVEY==201701)].NUMBERS.sum())\n",
    "print('2019 Pollock:',a[(a.SURVEY==201901)&(a.SPECIES_CODE.isin([21740,21744]))].NUMBERS.sum()/a[(a.SURVEY==201901)].NUMBERS.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201701 23041 1.616048700222718\n",
      "201701 21744 7.47572120769931\n",
      "201701 21725 89.81007433537224\n",
      "201701 21720 0.9413142382774418\n",
      "201701 21735 0.46004876872346534\n",
      "201701 21110 0.0\n",
      "201701 21740 0.0\n",
      "201901 23041 0.37398423386177304\n",
      "201901 21744 5.740441522294378\n",
      "201901 21725 12.55557445745253\n",
      "201901 21720 0.08921857967994705\n",
      "201901 21735 0.18837154481527138\n",
      "201901 21110 0.31742012372610956\n",
      "201901 21740 0.0012176257066017209\n"
     ]
    }
   ],
   "source": [
    "#Abundances by survey by species\n",
    "for surv in a.SURVEY.unique():\n",
    "    for spec in a.SPECIES_CODE.unique():\n",
    "        print(surv,spec,a[(a.SURVEY==surv)&(a.SPECIES_CODE==spec)].NUMBERS.sum()/10**10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012 Acod: 3.535163790827692\n",
      "2013 Acod: 3.5679015478588365\n",
      "2017 Acod: 4.408042066718216\n",
      "2019 Acod: 4.808930212883881\n",
      "2012 pollock: 4.978815338917016\n",
      "2013 pollock: 6.872612340325574\n",
      "2017 pollock: 5.107030595963149\n",
      "2019 pollock: 5.068405358275401\n"
     ]
    }
   ],
   "source": [
    "# lengths of arctic cod and pollock\n",
    "df2012 = pd.read_csv('../data/catchData/2012_2013/Arctic_EIS_AT_survey_Arctic_cod_by_length_alongtrack_fish_per_m_squared_2012_v2.csv')\n",
    "df2013 = pd.read_csv('../data/catchData/2012_2013/Arctic_EIS_AT_survey_Arctic_cod_by_length_alongtrack_fish_per_m_squared_2013_v2.csv')\n",
    "df2017 = pd.read_csv('../data/catchData/2017_2019/Arctic_EIS_201701_21725.csv')\n",
    "df2019 = pd.read_csv('../data/catchData/2017_2019/Arctic_EIS_201901_21725.csv')\n",
    "print('2012 Acod:',sum(df2012[df2012.Latitude >=65.9].sum()[3:]/sum(df2012[df2012.Latitude >=65.9].sum()[3:])*np.arange(2,31,1)))\n",
    "print('2013 Acod:',sum(df2013[df2013.Latitude >=65.9].sum()[3:]/sum(df2013[df2013.Latitude >=65.9].sum()[3:])*np.arange(2,31,1)))\n",
    "print('2017 Acod:',sum(df2017.sum()[3:23]/sum(df2017.sum()[3:23])*np.arange(1,19,1)))\n",
    "print('2019 Acod:',sum(df2019.sum()[3:23]/sum(df2019.sum()[3:23])*np.arange(1,21,1)))\n",
    "\n",
    "#pollock\n",
    "df2012 = pd.read_csv('../data/catchData/2012_2013/Arctic_EIS_AT_survey_pollock_by_length_alongtrack_fish_per_m_squared_2012_for_IERP.csv')\n",
    "df2013 = pd.read_csv('../data/catchData/2012_2013/Arctic_EIS_AT_survey_pollock_by_length_alongtrack_fish_per_m_squared_2013_for_IERP.csv')\n",
    "df2017 = pd.read_csv('../data/catchData/2017_2019/Arctic_EIS_201701_21744.csv')\n",
    "df2019 = pd.read_csv('../data/catchData/2017_2019/Arctic_EIS_201901_21744.csv')\n",
    "print('2012 pollock:',sum(df2012.sum()[3:23]/sum(df2012.sum()[3:23])*np.arange(2,22,1)))\n",
    "print('2013 pollock:',sum(df2013.sum()[3:23]/sum(df2013.sum()[3:23])*np.arange(2,22,1)))\n",
    "print('2017 pollock:',sum(df2017.sum()[3:23]/sum(df2017.sum()[3:23])*np.arange(1,19,1)))\n",
    "print('2019 pollock:',sum(df2019.sum()[3:23]/sum(df2019.sum()[3:23])*np.arange(1,21,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proportion of acoustic backscatter attributed to the key species\n",
    "```sql\n",
    "select a.survey, b.event_id,a.interval,  a.layer,a.prc_nasc, c.species_code, c.sa_proportion\n",
    "from \n",
    "(select * from integration_results) a\n",
    "join\n",
    "(select * from v_mean_sigma_by_interval_class) b\n",
    "on a.interval = b.interval\n",
    "and a.ship = b.ship\n",
    "and a.survey = b.survey\n",
    "join\n",
    "(select * from scaling_key_sa_by_species) c\n",
    "on b.event_id = c.event_id\n",
    "and b.ship = c.ship\n",
    "and b.survey = c.survey\n",
    "where a.ship = 175 and (a.survey=201701 or a.survey = 201901)\n",
    "order by a.interval, c.species_code\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SPECIES_CODE</th>\n",
       "      <th>NASC_SPECIES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>21725</td>\n",
       "      <td>2.121468e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>21744</td>\n",
       "      <td>9.728182e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>21735</td>\n",
       "      <td>2.225326e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>21720</td>\n",
       "      <td>1.515828e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>23041</td>\n",
       "      <td>8.660144e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>23808</td>\n",
       "      <td>8.464975e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>23801</td>\n",
       "      <td>2.520251e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>21334</td>\n",
       "      <td>9.884667e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>21368</td>\n",
       "      <td>5.670676e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>23800</td>\n",
       "      <td>5.363116e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SPECIES_CODE  NASC_SPECIES\n",
       "14         21725  2.121468e+06\n",
       "16         21744  9.728182e+05\n",
       "15         21735  2.225326e+05\n",
       "13         21720  1.515828e+05\n",
       "19         23041  8.660144e+04\n",
       "25         23808  8.464975e+04\n",
       "21         23801  2.520251e+04\n",
       "8          21334  9.884667e+03\n",
       "9          21368  5.670676e+03\n",
       "20         23800  5.363116e+03"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prop key spec:  0.959912798031704\n",
      "Prop gadid:  0.9365288980256332\n",
      "2019\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SPECIES_CODE</th>\n",
       "      <th>NASC_SPECIES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21744</td>\n",
       "      <td>1.860306e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>21725</td>\n",
       "      <td>8.055948e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>21735</td>\n",
       "      <td>2.968549e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>21110</td>\n",
       "      <td>1.710237e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>23807</td>\n",
       "      <td>1.026548e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>23041</td>\n",
       "      <td>8.184877e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>21720</td>\n",
       "      <td>5.376915e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>23800</td>\n",
       "      <td>3.123052e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21740</td>\n",
       "      <td>3.100961e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>40504</td>\n",
       "      <td>2.907665e+04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SPECIES_CODE  NASC_SPECIES\n",
       "21         21744  1.860306e+06\n",
       "18         21725  8.055948e+05\n",
       "19         21735  2.968549e+05\n",
       "9          21110  1.710237e+05\n",
       "31         23807  1.026548e+05\n",
       "27         23041  8.184877e+04\n",
       "17         21720  5.376915e+04\n",
       "29         23800  3.123052e+04\n",
       "20         21740  3.100961e+04\n",
       "33         40504  2.907665e+04"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prop key spec:  0.927883722923627\n",
      "Prop gadid:  0.8567905930082398\n"
     ]
    }
   ],
   "source": [
    "a = pd.read_csv('../data/acousticData/2017_2019/mbaSpeciesProp.csv')\n",
    "a['NASC_SPECIES'] = a.PRC_NASC*a.SA_PROPORTION\n",
    "\n",
    "c = a[a.SURVEY==201701]\n",
    "b = c.groupby(by=['SPECIES_CODE']).sum().reset_index()\n",
    "print('2017')\n",
    "display(b.sort_values(by='NASC_SPECIES',ascending=False)[['SPECIES_CODE','NASC_SPECIES']].head(10))\n",
    "specs = [21110, 23041, 21725, 21744, 21740, 21735, 21720]\n",
    "b[b.SPECIES_CODE.isin(specs)].NASC_SPECIES.sum()/b.NASC_SPECIES.sum()\n",
    "print('Prop key spec: ',b[b.SPECIES_CODE.isin(specs)].NASC_SPECIES.sum()/b.NASC_SPECIES.sum())\n",
    "specs = [21725, 21744, 21740, 21735, 21720]\n",
    "print('Prop gadid: ',b[b.SPECIES_CODE.isin(specs)].NASC_SPECIES.sum()/b.NASC_SPECIES.sum())\n",
    "\n",
    "c = a[a.SURVEY==201901]\n",
    "print('2019')\n",
    "b = c.groupby(by=['SPECIES_CODE']).sum().reset_index()\n",
    "display(b.sort_values(by='NASC_SPECIES',ascending=False)[['SPECIES_CODE','NASC_SPECIES']].head(10))\n",
    "specs = [21110, 23041, 21725, 21744, 21740, 21735, 21720]\n",
    "print('Prop key spec: ',b[b.SPECIES_CODE.isin(specs)].NASC_SPECIES.sum()/b.NASC_SPECIES.sum())\n",
    "specs = [21725, 21744, 21740, 21735, 21720]\n",
    "print('Prop gadid: ',b[b.SPECIES_CODE.isin(specs)].NASC_SPECIES.sum()/b.NASC_SPECIES.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total number of pollock found in catch north of the strait in 2012 and 2013 in the catch\n",
    "```sql \n",
    "select b.eq_latitude, a.*\n",
    "from\n",
    "(select * from v_aeis_catch_summary_v2 where survey=2012001 and (species_code = 21740 or species_code = 21744)) a\n",
    "join\n",
    "(select * from v_event_data) b\n",
    "on a.ship = b.ship and a.survey = b.survey and a.clams_event_number = b.event_id\n",
    "order by eq_latitude\n",
    "```\n",
    "\n",
    "28 in 2012, 2 in 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStat = pd.read_csv('catchAnalysis/analysisFiles/dfStat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.577283333333334 -0.7174166666666667\n",
      "34.73803333333333 31.094583333333336\n"
     ]
    }
   ],
   "source": [
    "#dfStat[dfStat.Year == 2017].T_surf.mean()\n",
    "print(dfStat[dfStat.Year == 2017].T_bot.max(),dfStat[dfStat.Year == 2017].T_bot.min())\n",
    "print(dfStat[dfStat.Year == 2017].S_bot.max(),dfStat[dfStat.Year == 2017].S_bot.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Genetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 genetics done:  755\n",
      "2017 all specimen:  2244\n",
      "2019 genetics done:  2983\n",
      "2019 all specimen:  5654\n"
     ]
    }
   ],
   "source": [
    "# This is not up to date...\n",
    "a = pd.read_csv('C:/Users/robert.levine/Work/repositories/ArcticEISII/catchProcessing/code/specimenCorrections/geneticID/data/gadidsUpdatedLen.csv')\n",
    "a = a[(a.GEAR=='Marinovich')&(a.PARTITION=='Codend')]\n",
    "print('2017 genetics done: ',len(a[(a.SURVEY==201701) & ((a.SPECIES_ID_METHOD=='genetics_changed')|(a.SPECIES_ID_METHOD=='genetics_confirmed'))]))\n",
    "print('2017 all specimen: ',len(a[(a.SURVEY==201701)]))\n",
    "\n",
    "print('2019 genetics done: ',len(a[(a.SURVEY==201901) & ((a.SPECIES_ID_METHOD=='genetics_changed')|(a.SPECIES_ID_METHOD=='genetics_confirmed'))]))\n",
    "print('2019 all specimen: ',len(a[(a.SURVEY==201901)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql select count(*) from v_aeis_specimen\n",
    "where survey = 201701 and\n",
    "gear = 'Marinovich'\n",
    "and net_partition = 'Codend'\n",
    "and species_code < 40000\n",
    "\n",
    "3392\n",
    "\n",
    "and species_code > 40000\n",
    "\n",
    "1211\n",
    "\n",
    "select count(*) from v_aeis_specimen\n",
    "where survey = 201701 and\n",
    "gear = 'Marinovich'\n",
    "and net_partition = 'Codend'\n",
    "and (species_code > 21700\n",
    "and species_code < 21800\n",
    "or species_code = 1202)\n",
    "order by species_code\n",
    "\n",
    "2244\n",
    "\n",
    "select count(*) from v_aeis_specimen\n",
    "where survey = 201701 and\n",
    "gear = 'Marinovich'\n",
    "and net_partition = 'Codend'\n",
    "and (species_id_method = 'genetics_confirmed'\n",
    "or species_id_method = 'genetics_changed')\n",
    "\n",
    "894\n",
    "\n",
    "select count(*) from v_aeis_specimen\n",
    "where survey = 201701 and\n",
    "gear = 'Marinovich'\n",
    "and net_partition = 'Codend'\n",
    "and species_id_method = 'genetics_model_assigned'\n",
    "\n",
    "1350\n",
    "\n",
    "select count(*) from v_aeis_specimen\n",
    "where survey = 201701 and\n",
    "gear = 'Marinovich'\n",
    "and net_partition = 'Codend'\n",
    "and species_id_method = 'field_id'\n",
    "\n",
    "0\n",
    "\n",
    "\n",
    "select count(*) from v_aeis_specimen\n",
    "where survey = 201901 and\n",
    "gear = 'Marinovich'\n",
    "and net_partition = 'Codend'\n",
    "and species_code < 40000\n",
    "\n",
    "9124\n",
    "\n",
    "and species_code > 40000\n",
    "\n",
    "751\n",
    "\n",
    "select count(*) from v_aeis_specimen\n",
    "where survey = 201901 and\n",
    "gear = 'Marinovich'\n",
    "and net_partition = 'Codend'\n",
    "and (species_code > 21700\n",
    "and species_code < 21800\n",
    "or species_code = 1202)\n",
    "order by species_code\n",
    "\n",
    "5676\n",
    "\n",
    "\n",
    "select count(*) from v_aeis_specimen\n",
    "where survey = 201901 and\n",
    "gear = 'Marinovich'\n",
    "and net_partition = 'Codend'\n",
    "and (species_id_method = 'genetics_confirmed'\n",
    "or species_id_method = 'genetics_changed')\n",
    "\n",
    "3155\n",
    "\n",
    "select count(*) from v_aeis_specimen\n",
    "where survey = 201901 and\n",
    "gear = 'Marinovich'\n",
    "and species_id_method = 'genetics_model_assigned'\n",
    "\n",
    "2488\n",
    "\n",
    "select count(*) from v_aeis_specimen\n",
    "where survey = 201901 and\n",
    "gear = 'Marinovich'\n",
    "and net_partition = 'Codend'\n",
    "and species_id_method = 'field_id'\n",
    "\n",
    "11\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
