{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order of operations\n",
    "\n",
    "1. Gadid specimen get in CLAMS2ABL based on genetic IDs provided by Sharon Wildes (ArcticEIS repo, specimenCorrections modelLen.py)\n",
    "2. Run MBA for 201701 and 201901\n",
    "3. Export the following tables:\n",
    "    - catchData\n",
    "        - mbaCatchResults (mbaCatchResults.sql)\n",
    "        - v_aeis_catch_summary_v2\n",
    "        - v_aeis_events\n",
    "        - v_aeis_specimen\n",
    "            - Build trawl selectivity tables:\n",
    "                - Add consistent lengths for all gadid specimen (Arctic EIS repo, specimenCorrections addCommonLength.py)\n",
    "                - Export Basket and sample tables from CLAMS2ABL:\n",
    "                    `select * from baskets where (ship = 175 and (survey = 201701 or survey = 201901)) or (ship = 174)`\n",
    "                    `select * from samples where (ship = 175 and (survey = 201701 or survey = 201901)) or (ship = 174)`\n",
    "                - Add expansion values to the table (Arctic EIS repo, trawlSelectivity buildSelectivityTable.ipynb)\n",
    "        - AIERP_EventData (ArcticEIS, EventExport.sql)\n",
    "            - Calculate volme filtered (aeisEvents EventData.ipynb)\n",
    "    - acousticData\n",
    "        - mbaIntegrationResults (mbaResultsExport.sql)\n",
    "        - mbaSpeciesProp (see SQL code below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 transit speed (m/s):  3.3321535976596404\n",
      "102426.0100799973\n",
      "2019 transit speed (m/s):  3.3983184608848283\n",
      "123334.0487099994\n"
     ]
    }
   ],
   "source": [
    "# Transit speed\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "files = glob('../data/acousticData/2017_2019/EV/2017/Echoview/exports/5m/*(intervals).csv')+ glob('../data/acousticData/2017_2019/EV/2017/Echoview/exports/5m_long//*(intervals).csv')\n",
    "a = pd.concat(map(lambda file: pd.read_csv(file), files))\n",
    "a['Datetime_S'] = pd.to_datetime(a['Date_S'].astype(str)+a['Time_S'])\n",
    "a['Datetime_E'] = pd.to_datetime(a['Date_E'].astype(str)+a['Time_E'])\n",
    "a['Duration'] = (a.Datetime_E- a.Datetime_S).dt.total_seconds()\n",
    "a['Dist'] = a['VL_end']-a['VL_start']\n",
    "a['Speed'] = a.Dist*1852/a.Duration\n",
    "print('2017 transit speed (m/s): ',a.Speed.mean())\n",
    "print(sum((a.Dist)*(30)))\n",
    "\n",
    "from glob import glob\n",
    "files = glob('../data/acousticData/2017_2019/EV/2019/Echoview/exports/5m/*(intervals).csv')+ glob('../data/acousticData/2017_2019/EV/2019/Echoview/exports/5m_long//*(intervals).csv')\n",
    "a = pd.concat(map(lambda file: pd.read_csv(file), files))\n",
    "a['Datetime_S'] = pd.to_datetime(a['Date_S'].astype(str)+' '+a['Time_S'])\n",
    "a['Datetime_E'] = pd.to_datetime(a['Date_E'].astype(str)+' '+a['Time_E'])\n",
    "a['Duration'] = (a.Datetime_E- a.Datetime_S).dt.total_seconds()\n",
    "a['Dist'] = a['VL_end']-a['VL_start']\n",
    "a['Speed'] = a.Dist*1852/a.Duration\n",
    "print('2019 transit speed (m/s): ',a.Speed.mean())\n",
    "print(sum((a.Dist)*(45)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Geodesic area 2017: 148994.181 km^2\n",
      "# Geodesic area 2019: 153995.194 km^2\n",
      "overlap in 2017: 0.9195896511652564\n",
      "overlap in 2019: 0.9222009870487491\n"
     ]
    }
   ],
   "source": [
    "from pyproj import Geod\n",
    "from shapely import wkt\n",
    "\n",
    "# specify a named ellipsoid\n",
    "# 2017\n",
    "geod = Geod(ellps=\"WGS84\")\n",
    "poly1 = wkt.loads('''\\\n",
    "POLYGON ((-168.60846216444355 66.99715863755357,-168.7086763789516 72.08417422880358,-165.9694878490645 72.08417422880358,-165.3682025620161 72.53302854567858,-157.41787487770966 72.57577657585715,-157.0170180196774 71.95593013826786,-157.65170804489514 70.97272544416072,\n",
    "-160.55792026562904 70.35287900657143,-163.93179882073386 69.946772719875,-164.13222724975 69.5192924180893,-166.40374944526613 68.49333969380356,-165.9026783727258 68.04448537692856,-164.46627463144355 67.55288302987499,-164.70010779862903 66.954410607375,-168.60846216444355 66.99715863755357))''')\n",
    "area = abs(geod.geometry_area_perimeter(poly1)[0])/1e+6\n",
    "print('# Geodesic area 2017: {:.3f} km^2'.format(area))\n",
    "\n",
    "poly2 = wkt.loads('''\\\n",
    "POLYGON ((-168.60107787551613 66.49453132622024,-168.60107787551613 72.5133516521131,-160.24014568166936 72.47401295717262,-159.32608878587902 72.98541599139881,-156.79899030928226 72.65103708440476,-157.12159862544354 71.76591644824404,-157.49797499429837 70.95947320196429,\n",
    "-163.95014131752419 70.192368650625,-164.08456144925805 69.5236108366369,-165.42876276659678 69.18923192964286,-166.93426824201612 68.81551432770833,-166.77296408393548 68.40245803083333,-165.75137108275806 68.12708716624999,-164.5684739235 67.22229718261904,-165.99332731987903 66.41585393633929,-168.60107787551613 66.49453132622024))''')\n",
    "area = abs(geod.geometry_area_perimeter(poly2)[0])/(1000*1000)\n",
    "print('# Geodesic area 2019: {:.3f} km^2'.format(area))\n",
    "print('overlap in 2017:',1-abs(geod.geometry_area_perimeter(poly1.difference(poly2))[0])/abs(geod.geometry_area_perimeter(poly1)[0]))\n",
    "print('overlap in 2019:',1-abs(geod.geometry_area_perimeter(poly1.difference(poly2))[0])/abs(geod.geometry_area_perimeter(poly2)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43439.71422095542"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "148994.181/(1/0.29155309240537)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading files...\n",
      "mean bottom depth:  45.951389172451705\n",
      "min bottom depth:  10.293944\n",
      "max bottom depth:  278.056195\n",
      "% shallower than  50  m:  76.12690435055757\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "def bottomDepths(files,refDepth):\n",
    "    print('reading files...')\n",
    "    hold = []\n",
    "    for filename in files:\n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        hold.append(df)\n",
    "    df = pd.concat(hold, axis=0, ignore_index=True)\n",
    "    bD = df.Exclude_below_line_depth_mean.values\n",
    "    bD[bD == -9999.0] = np.nan\n",
    "    bD = bD+0.5\n",
    "    print('mean bottom depth: ',np.nanmean(bD))\n",
    "    print('min bottom depth: ',np.nanmin(bD))\n",
    "    print('max bottom depth: ',np.nanmax(bD))\n",
    "    print('% shallower than ',refDepth,' m: ',(len(np.where(bD < refDepth)[0])/len(bD))*100)\n",
    "    plt.hist(bD, bins=100, density=True)\n",
    "    return df\n",
    "\n",
    "files17 = glob('../data/acousticData/2017_2019/EV/2017/Echoview/exports/5m/*(intervals).csv')\n",
    "files19 = glob('../data/acousticData/2017_2019/EV/2019/Echoview/exports/5m/*(intervals).csv')\n",
    "bD = bottomDepths(files17,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017, 38 @ .512:  25.117 -0.855\n",
      "2019, 38 @ .512:  25.16 -0.8\n",
      "0.02231264103257835\n",
      "2017, 38 @ 4.0:  25.32 -0.24\n",
      "2019, 38 @ 4.0:  25.367 -0.28\n",
      "0.0016105112976727565\n"
     ]
    }
   ],
   "source": [
    "# Calibration\n",
    "# From the E:\\ChukchiTimeSeries\\data\\acousticData\\2017_2019\\EV\\2019\\calibration\\Ocean Starr EK60 cals AFSC.xslx spreadhseet\n",
    "\n",
    "# 38 kHz 512\n",
    "# 2017 \n",
    "# Cal1: 25.23, -0.88\n",
    "# Cal2: 25.00, -0.83\n",
    "print('2017, 38 @ .512: ',round(10*np.log10(((10**(25.23/10))+(10**(25.00/10)))/2),3),round(10*np.log10(((10**(-.88/10))+(10**(-.83/10)))/2),3))\n",
    "\n",
    "# 2019\n",
    "# Cal1: 25.17, -0.84\n",
    "# Cal2: 25.15, -0.76\n",
    "print('2019, 38 @ .512: ',round(10*np.log10(((10**(25.17/10))+(10**(25.15/10)))/2),3),round(10*np.log10(((10**(-.84/10))+(10**(-.76/10)))/2),3))\n",
    "\n",
    "\n",
    "print(1-(10**((25.117-.855)/10))/(10**((25.16-.8)/10)))\n",
    "\n",
    "# 38 kHz 4\n",
    "# 2017 (only 1 cal) 25.32,-0.24\n",
    "print('2017, 38 @ 4.0: ',25.32,-0.24)\n",
    "\n",
    "# 2019\n",
    "# Cal1: 25.25, -0.28\n",
    "# Cal2: 25.48, -0.28\n",
    "print('2019, 38 @ 4.0: ',round(10*np.log10(((10**(25.25/10))+(10**(25.48/10)))/2),3),round(10*np.log10(((10**(-.28/10))+(10**(-.28/10)))/2),3))\n",
    "\n",
    "print(1-(10**((25.32 -0.24)/10))/(10**((25.367 -0.28)/10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017, 120 @ .512:  24.79 -0.46\n",
      "2019, 120 @ .512:  24.959 -0.45\n",
      "0.04037843288965892\n",
      "2017, 120 @ 1:  24.56 -0.39\n",
      "2019, 120 @ 1:  24.933 -0.355\n",
      "0.08966760032009014\n"
     ]
    }
   ],
   "source": [
    "# 120 kHz 512\n",
    "# 2017 \n",
    "# Cal1: 25.07, -0.41\n",
    "# Cal2: 24.49, -0.51\n",
    "print('2017, 120 @ .512: ',round(10*np.log10(((10**(25.07/10))+(10**(24.49/10)))/2),3),round(10*np.log10(((10**(-.41/10))+(10**(-.51/10)))/2),3))\n",
    "\n",
    "# 2019\n",
    "# Cal1: 25.39, -0.46\n",
    "# Cal2: 24.48, -0.44\n",
    "print('2019, 120 @ .512: ',round(10*np.log10(((10**(25.39/10))+(10**(24.48/10)))/2),3),round(10*np.log10(((10**(-.46/10))+(10**(-.44/10)))/2),3))\n",
    "\n",
    "print(1-(10**((24.79 -0.46)/10))/(10**((24.959 -0.45)/10)))\n",
    "\n",
    "# 120 kHz 1\n",
    "# 2017 (only 1 cal) 24.56, -0.39\n",
    "print('2017, 120 @ 1: ',24.56, -0.39)\n",
    "\n",
    "# 2019\n",
    "# Cal1: 25.36, -0.35\n",
    "# Cal2: 24.46, -0.36\n",
    "print('2019, 120 @ 1: ',round(10*np.log10(((10**(25.36/10))+(10**(24.46/10)))/2),3),round(10*np.log10(((10**(-.35/10))+(10**(-.36/10)))/2),3))\n",
    "\n",
    "print(1-(10**((24.56 -0.39)/10))/(10**((24.933 -0.355)/10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 vert opening +STD:  8.280166666666666 1.12605543084403\n",
      "2017 hori opening +STD:  7.559833333333334 0.9346034410133652\n",
      "2019 vert opening +STD:  7.533414634146343 0.5061650410493478\n",
      "2019 hori opening +STD:  7.449062499999999 0.3429413509060847\n",
      "2017 HR mean, min, max : 27.12212121212121 11.46 46.78\n",
      "2019 HR mean, min, max : 34.98837209302326 13.27 227.9\n"
     ]
    }
   ],
   "source": [
    "# Net Openings\n",
    "b = pd.read_csv('../data/catchData/2017_2019/AIERP_EventData.csv')\n",
    "print('2017 vert opening +STD: ',b[b.SURVEY==201701].AVG_NET_VERT_OPENING.mean(),b[b.SURVEY==201701].AVG_NET_VERT_OPENING.std())\n",
    "print('2017 hori opening +STD: ',b[b.SURVEY==201701].AVG_NET_HORI_OPENING.mean(),b[b.SURVEY==201701].AVG_NET_HORI_OPENING.std())\n",
    "print('2019 vert opening +STD: ',b[b.SURVEY==201901].AVG_NET_VERT_OPENING.mean(),b[b.SURVEY==201901].AVG_NET_VERT_OPENING.std())\n",
    "print('2019 hori opening +STD: ',b[b.SURVEY==201901].AVG_NET_HORI_OPENING.mean(),b[b.SURVEY==201901].AVG_NET_HORI_OPENING.std())\n",
    "print('2017 HR mean, min, max :',b[b.SURVEY==201701].AVG_HEAD_ROPE_DEPTH.mean(),b[b.SURVEY==201701].AVG_HEAD_ROPE_DEPTH.min(),b[b.SURVEY==201701].AVG_HEAD_ROPE_DEPTH.max())\n",
    "print('2019 HR mean, min, max :',b[b.SURVEY==201901].AVG_HEAD_ROPE_DEPTH.mean(),b[b.SURVEY==201901].AVG_HEAD_ROPE_DEPTH.min(),b[b.SURVEY==201901].AVG_HEAD_ROPE_DEPTH.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# CTD per year:  68 55 39 48\n"
     ]
    }
   ],
   "source": [
    "# Number of CTD Stations\n",
    "c = pd.read_csv('catchAnalysis/analysisFiles/dfStat.csv')\n",
    "c = c[(~c.meanSa.isnull())&(~c.meanAcod.isnull())&(~c.meanPol.isnull())&(~c.meanCap.isnull())]\n",
    "print('# CTD per year: ',len(c[c.Year == 2012]),len(c[c.Year == 2013]),len(c[c.Year == 2017]),len(c[c.Year == 2019]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 All fishes  0.24819964689893076\n",
      "2017 All Arctic Cod  0.18904386261152697\n",
      "2019 All fishes  0.034814017790898\n",
      "2019 All Arctic Cod  0.01747772209291293\n"
     ]
    }
   ],
   "source": [
    "# CPUE\n",
    "dfEvents = pd.read_csv('../data/catchData/2017_2019/AIERP_EventData.csv')\n",
    "catch = pd.read_csv('../data/catchData/2017_2019/catchExport.csv')\n",
    "events = pd.read_csv('../data/catchData/2017_2019/eventExport.csv')\n",
    "\n",
    "dfCatch = catch[(catch.GEAR == 'Marinovich') & (catch.SPECIES_CODE <40000) & (catch.SURVEY==201701)].merge(dfEvents, how='left',  left_on=['SURVEY','CLAMS_EVENT_NUMBER'],   right_on=['SURVEY','EVENT_ID'])\n",
    "dfCatch['CPUE'] = dfCatch.TOTAL_NUMBER_IN_HAUL/dfCatch.VOL_FILTERED\n",
    "dfCatch = dfCatch.groupby(by=['CLAMS_EVENT_NUMBER']).sum()\n",
    "print('2017 All fishes ',dfCatch.CPUE.mean())\n",
    "\n",
    "\n",
    "dfCatch = catch[(catch.GEAR == 'Marinovich') & (catch.SPECIES_CODE ==21725) & (catch.SURVEY==201701)].merge(dfEvents, how='left',  left_on=['SURVEY','CLAMS_EVENT_NUMBER'],   right_on=['SURVEY','EVENT_ID'])\n",
    "dfCatch['CPUE'] = dfCatch.TOTAL_NUMBER_IN_HAUL/dfCatch.VOL_FILTERED\n",
    "dfCatch = dfCatch.groupby(by=['CLAMS_EVENT_NUMBER']).sum()\n",
    "print('2017 All Arctic Cod ',dfCatch.CPUE.mean())\n",
    "\n",
    "dfCatch = catch[(catch.GEAR == 'Marinovich') & (catch.SPECIES_CODE <40000) & (catch.SURVEY==201901)].merge(dfEvents, how='left',  left_on=['SURVEY','CLAMS_EVENT_NUMBER'],   right_on=['SURVEY','EVENT_ID'])\n",
    "dfCatch['CPUE'] = dfCatch.TOTAL_NUMBER_IN_HAUL/dfCatch.VOL_FILTERED\n",
    "dfCatch = dfCatch.groupby(by=['CLAMS_EVENT_NUMBER']).sum()\n",
    "print('2019 All fishes ',dfCatch.CPUE.mean())\n",
    "\n",
    "\n",
    "dfCatch = catch[(catch.GEAR == 'Marinovich') & (catch.SPECIES_CODE ==21725) & (catch.SURVEY==201901)].merge(dfEvents, how='left',  left_on=['SURVEY','CLAMS_EVENT_NUMBER'],   right_on=['SURVEY','EVENT_ID'])\n",
    "dfCatch['CPUE'] = dfCatch.TOTAL_NUMBER_IN_HAUL/dfCatch.VOL_FILTERED\n",
    "dfCatch = dfCatch.groupby(by=['CLAMS_EVENT_NUMBER']).sum()\n",
    "print('2019 All Arctic Cod ',dfCatch.CPUE.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 length of 80 percentile:  7.3\n",
      "2019 length of 80 percentile:  8.0\n"
     ]
    }
   ],
   "source": [
    "# Lengths from catch\n",
    "a = pd.read_csv('../data/catchData/2017_2019/specimen_complete_selectivity.csv')\n",
    "import matplotlib.pyplot as plt\n",
    "b = a[(a.SURVEY==201701)&(~a.CONSISTENT_LENGTH.isnull())].CONSISTENT_LENGTH.values\n",
    "print('2017 length of 80 percentile: ',np.percentile(b,80))\n",
    "b = a[(a.SURVEY==201901)&(~a.CONSISTENT_LENGTH.isnull())].CONSISTENT_LENGTH.values\n",
    "print('2019 length of 80 percentile: ',np.percentile(b,80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012\n",
      "% fish abundance: 0.9522844828607854\n",
      "% Gad,cap,her abundance: 0.9472639864899757\n",
      "% fish weight: 0.44809381387314423\n",
      "% jelly abundance: 0.02619677417103569\n",
      "% jelly weight: 0.5291975118480747\n",
      "C. melanaster:  0.9896329107693402\n",
      "2013\n",
      "% fish abundance: 0.8730348247786125\n",
      "% Gad,cap,her abundance: 0.8386735000776655\n",
      "% fish weight: 0.18414039341960065\n",
      "% jelly abundance: 0.03985783583825937\n",
      "% jelly weight: 0.7597752247689786\n",
      "C. melanaster:  0.9404935175553626\n"
     ]
    }
   ],
   "source": [
    "# Abundance and biomass from catch\n",
    "a = pd.read_csv('../data/catchData/2017_2019/catchExport.csv')\n",
    "a = a[((a.GEAR == 'Marinovich')|(a.GEAR == 'CanTrawl')) & (a.NET_PARTITION == 'Codend')]\n",
    "a = a[a.SURVEY == 2012001]\n",
    "a = a[(a.SPECIES_CODE >=1200)&(a.SPECIES_CODE < 60000)]\n",
    "aJel = np.unique(a[(a.SPECIES_CODE > 40000) & (a.SPECIES_CODE  < 50000)].SPECIES_CODE.values)# jellyfish\n",
    "aGad = np.unique( a[((a.SPECIES_CODE> 21700) & (a.SPECIES_CODE < 21750)) | (a.SPECIES_CODE == 1202)].SPECIES_CODE.values) # gadids    \n",
    "aSan =  np.unique(a[(a.SPECIES_CODE >= 20202) & (a.SPECIES_CODE <= 20204)].SPECIES_CODE.values) # sand lance\n",
    "aStic =  np.unique(a[(a.SPECIES_CODE >= 23800) & (a.SPECIES_CODE <= 23810)].SPECIES_CODE.values) # prickleback\n",
    "aCap =  np.unique(a[(a.SPECIES_CODE== 23041)].SPECIES_CODE.values) # capelin\n",
    "aHer =  np.unique(a[(a.SPECIES_CODE == 21110)].SPECIES_CODE.values) # herring\n",
    "print('2012')\n",
    "print('% fish abundance:', a[a.SPECIES_CODE.isin(np.concatenate([aGad,aSan,aStic,aCap,aHer]))].TOTAL_NUMBER_IN_HAUL.sum()/a.TOTAL_NUMBER_IN_HAUL.sum())\n",
    "print('% Gad,cap,her abundance:', a[a.SPECIES_CODE.isin(np.concatenate([aGad,aCap,aHer]))].TOTAL_NUMBER_IN_HAUL.sum()/a.TOTAL_NUMBER_IN_HAUL.sum())\n",
    "print('% fish weight:', a[a.SPECIES_CODE.isin(np.concatenate([aGad,aSan,aStic,aCap,aHer]))].TOTAL_WEIGHT_IN_HAUL.sum()/a.TOTAL_WEIGHT_IN_HAUL.sum())\n",
    "print('% jelly abundance:', a[a.SPECIES_CODE.isin(aJel)].TOTAL_NUMBER_IN_HAUL.sum()/a.TOTAL_NUMBER_IN_HAUL.sum())\n",
    "print('% jelly weight:', a[a.SPECIES_CODE.isin(aJel)].TOTAL_WEIGHT_IN_HAUL.sum()/a.TOTAL_WEIGHT_IN_HAUL.sum())\n",
    "print('C. melanaster: ',a[a.SPECIES_CODE==40504].TOTAL_WEIGHT_IN_HAUL.sum()/a[a.SPECIES_CODE.isin(aJel)].TOTAL_WEIGHT_IN_HAUL.sum())\n",
    "\n",
    "\n",
    "# Abundance and biomass from catch\n",
    "a = pd.read_csv('../data/catchData/2017_2019/catchExport.csv')\n",
    "a = a[((a.GEAR == 'Marinovich')|(a.GEAR == 'CanTrawl')) & (a.NET_PARTITION == 'Codend')]\n",
    "a = a[a.SURVEY == 2013001]\n",
    "a = a[(a.SPECIES_CODE >=1200)&(a.SPECIES_CODE < 60000)]\n",
    "aJel = np.unique(a[(a.SPECIES_CODE > 40000) & (a.SPECIES_CODE  < 50000)].SPECIES_CODE.values)# jellyfish\n",
    "aGad = np.unique( a[((a.SPECIES_CODE> 21700) & (a.SPECIES_CODE < 21750)) | (a.SPECIES_CODE == 1202)].SPECIES_CODE.values) # gadids    \n",
    "aSan =  np.unique(a[(a.SPECIES_CODE >= 20202) & (a.SPECIES_CODE <= 20204)].SPECIES_CODE.values) # sand lance\n",
    "aStic =  np.unique(a[(a.SPECIES_CODE >= 23800) & (a.SPECIES_CODE <= 23810)].SPECIES_CODE.values) # prickleback\n",
    "aCap =  np.unique(a[(a.SPECIES_CODE== 23041)].SPECIES_CODE.values) # capelin\n",
    "aHer =  np.unique(a[(a.SPECIES_CODE == 21110)].SPECIES_CODE.values) # herring\n",
    "print('2013')\n",
    "print('% fish abundance:', a[a.SPECIES_CODE.isin(np.concatenate([aGad,aSan,aStic,aCap,aHer]))].TOTAL_NUMBER_IN_HAUL.sum()/a.TOTAL_NUMBER_IN_HAUL.sum())\n",
    "print('% Gad,cap,her abundance:', a[a.SPECIES_CODE.isin(np.concatenate([aGad,aCap,aHer]))].TOTAL_NUMBER_IN_HAUL.sum()/a.TOTAL_NUMBER_IN_HAUL.sum())\n",
    "print('% fish weight:', a[a.SPECIES_CODE.isin(np.concatenate([aGad,aSan,aStic,aCap,aHer]))].TOTAL_WEIGHT_IN_HAUL.sum()/a.TOTAL_WEIGHT_IN_HAUL.sum())\n",
    "print('% jelly abundance:', a[a.SPECIES_CODE.isin(aJel)].TOTAL_NUMBER_IN_HAUL.sum()/a.TOTAL_NUMBER_IN_HAUL.sum())\n",
    "print('% jelly weight:', a[a.SPECIES_CODE.isin(aJel)].TOTAL_WEIGHT_IN_HAUL.sum()/a.TOTAL_WEIGHT_IN_HAUL.sum())\n",
    "print('C. melanaster: ',a[a.SPECIES_CODE==40504].TOTAL_WEIGHT_IN_HAUL.sum()/a[a.SPECIES_CODE.isin(aJel)].TOTAL_WEIGHT_IN_HAUL.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017\n",
      "% fish abundance: 0.9863262432839877\n",
      "% Gad,cap,her abundance: 0.9627678054429277\n",
      "% fish weight: 0.16027996763123512\n",
      "% jelly abundance: 0.007696415930663662\n",
      "% jelly weight: 0.8383413655195447\n",
      "C. melanaster:  0.6088457142338402\n",
      "2019\n",
      "% fish abundance: 0.9343413869915407\n",
      "% Gad,cap,her abundance: 0.6372793522279826\n",
      "% fish weight: 0.06773924503088613\n",
      "% jelly abundance: 0.055354326542393625\n",
      "% jelly weight: 0.9312036004607144\n",
      "C. melanaster:  0.5839014769756139\n"
     ]
    }
   ],
   "source": [
    "# Abundance and biomass from catch\n",
    "a = pd.read_csv('../data/catchData/2017_2019/catchExport.csv')\n",
    "a = a[(a.GEAR == 'Marinovich') & (a.NET_PARTITION == 'Codend')]\n",
    "a = a[a.SURVEY == 201701]\n",
    "a = a[(a.SPECIES_CODE >=1200)&(a.SPECIES_CODE < 60000)]\n",
    "aJel = np.unique(a[(a.SPECIES_CODE > 40000) & (a.SPECIES_CODE  < 50000)].SPECIES_CODE.values)# jellyfish\n",
    "aGad = np.unique( a[((a.SPECIES_CODE> 21700) & (a.SPECIES_CODE < 21750)) | (a.SPECIES_CODE == 1202)].SPECIES_CODE.values) # gadids    \n",
    "aSan =  np.unique(a[(a.SPECIES_CODE >= 20202) & (a.SPECIES_CODE <= 20204)].SPECIES_CODE.values) # sand lance\n",
    "aStic =  np.unique(a[(a.SPECIES_CODE >= 23800) & (a.SPECIES_CODE <= 23810)].SPECIES_CODE.values) # prickleback\n",
    "aCap =  np.unique(a[(a.SPECIES_CODE== 23041)].SPECIES_CODE.values) # capelin\n",
    "aHer =  np.unique(a[(a.SPECIES_CODE == 21110)].SPECIES_CODE.values) # herring\n",
    "print('2017')\n",
    "print('% fish abundance:', a[a.SPECIES_CODE.isin(np.concatenate([aGad,aSan,aStic,aCap,aHer]))].TOTAL_NUMBER_IN_HAUL.sum()/a.TOTAL_NUMBER_IN_HAUL.sum())\n",
    "print('% Gad,cap,her abundance:', a[a.SPECIES_CODE.isin(np.concatenate([aGad,aCap,aHer]))].TOTAL_NUMBER_IN_HAUL.sum()/a.TOTAL_NUMBER_IN_HAUL.sum())\n",
    "print('% fish weight:', a[a.SPECIES_CODE.isin(np.concatenate([aGad,aSan,aStic,aCap,aHer]))].TOTAL_WEIGHT_IN_HAUL.sum()/a.TOTAL_WEIGHT_IN_HAUL.sum())\n",
    "print('% jelly abundance:', a[a.SPECIES_CODE.isin(aJel)].TOTAL_NUMBER_IN_HAUL.sum()/a.TOTAL_NUMBER_IN_HAUL.sum())\n",
    "print('% jelly weight:', a[a.SPECIES_CODE.isin(aJel)].TOTAL_WEIGHT_IN_HAUL.sum()/a.TOTAL_WEIGHT_IN_HAUL.sum())\n",
    "print('C. melanaster: ',a[a.SPECIES_CODE==40504].TOTAL_WEIGHT_IN_HAUL.sum()/a[a.SPECIES_CODE.isin(aJel)].TOTAL_WEIGHT_IN_HAUL.sum())\n",
    "\n",
    "a = pd.read_csv('../data/catchData/2017_2019/catchExport.csv')\n",
    "a = a[(a.GEAR == 'Marinovich') & (a.NET_PARTITION == 'Codend')]\n",
    "a = a[a.SURVEY == 201901]\n",
    "a = a[(a.SPECIES_CODE >=1200)&(a.SPECIES_CODE < 60000)]\n",
    "aJel = np.unique(a[(a.SPECIES_CODE > 40000) & (a.SPECIES_CODE  < 50000)].SPECIES_CODE.values)# jellyfish\n",
    "aGad = np.unique( a[((a.SPECIES_CODE> 21700) & (a.SPECIES_CODE < 21750)) | (a.SPECIES_CODE == 1202)].SPECIES_CODE.values) # gadids    \n",
    "aSan =  np.unique(a[(a.SPECIES_CODE >= 20202) & (a.SPECIES_CODE <= 20204)].SPECIES_CODE.values) # sand lance\n",
    "aStic =  np.unique(a[(a.SPECIES_CODE >= 23800) & (a.SPECIES_CODE <= 23810)].SPECIES_CODE.values) # prickleback\n",
    "aCap =  np.unique(a[(a.SPECIES_CODE== 23041)].SPECIES_CODE.values) # capelin\n",
    "aHer =  np.unique(a[(a.SPECIES_CODE == 21110)].SPECIES_CODE.values) # herring\n",
    "print('2019')\n",
    "print('% fish abundance:', a[a.SPECIES_CODE.isin(np.concatenate([aGad,aSan,aStic,aCap,aHer]))].TOTAL_NUMBER_IN_HAUL.sum()/a.TOTAL_NUMBER_IN_HAUL.sum())\n",
    "print('% Gad,cap,her abundance:', a[a.SPECIES_CODE.isin(np.concatenate([aGad,aCap,aHer]))].TOTAL_NUMBER_IN_HAUL.sum()/a.TOTAL_NUMBER_IN_HAUL.sum())\n",
    "print('% fish weight:', a[a.SPECIES_CODE.isin(np.concatenate([aGad,aSan,aStic,aCap,aHer]))].TOTAL_WEIGHT_IN_HAUL.sum()/a.TOTAL_WEIGHT_IN_HAUL.sum())\n",
    "print('% jelly abundance:', a[a.SPECIES_CODE.isin(aJel)].TOTAL_NUMBER_IN_HAUL.sum()/a.TOTAL_NUMBER_IN_HAUL.sum())\n",
    "print('% jelly weight:', a[a.SPECIES_CODE.isin(aJel)].TOTAL_WEIGHT_IN_HAUL.sum()/a.TOTAL_WEIGHT_IN_HAUL.sum())\n",
    "print('C. melanaster: ',a[a.SPECIES_CODE==40504].TOTAL_WEIGHT_IN_HAUL.sum()/a[a.SPECIES_CODE.isin(aJel)].TOTAL_WEIGHT_IN_HAUL.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.31331154280329\n",
      "220.85999182865228\n",
      "1094.893582625908\n",
      "178.53462277255952\n",
      "239.6171650003879\n",
      "2017 is X times higher than the other years:  15.571640114822221 4.957410228808434 6.132668082093662 4.569345366489649\n"
     ]
    }
   ],
   "source": [
    "# Backscatter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df2012Summary = pd.read_csv('../data/acousticData/2012_2013/Arctic_EIS_Acoustic_trawl_survey_alongtrack_summary_2012_v3.csv')\n",
    "df2013Summary = pd.read_csv('../data/acousticData/2012_2013/Arctic_EIS_Acoustic_trawl_survey_alongtrack_summary_2013_v3.csv')\n",
    "df2017Summary = pd.read_csv('../data/acousticData/2017_2019/Arctic_EIS_Acoustic_trawl_survey_alongtrack_summary_2017.csv')\n",
    "df2018Summary = pd.read_csv('../data/acousticData/2018/Arctic_EIS_Acoustic_trawl_survey_alongtrack_summary_2018.csv')\n",
    "df2019Summary = pd.read_csv('../data/acousticData/2017_2019/Arctic_EIS_Acoustic_trawl_survey_alongtrack_summary_2019.csv')\n",
    "print(np.mean(df2012Summary['Fish 38 kHz sA (m^2 nmi^-2)'][(df2012Summary.Latitude > 66) & (df2012Summary.Longitude < -156.7)]))# & (df2012Summary.Latitude  < 71.85)]))\n",
    "print(np.mean(df2013Summary['Fish 38 kHz sA (m^2 nmi^-2)'][(df2013Summary.Latitude > 66)& (df2013Summary.Longitude < -156.7)]))# & (df2013Summary.Latitude  < 71.85)]))\n",
    "print(np.mean(df2017Summary['Fish 38 kHz sA (m^2 nmi^-2)'][(df2017Summary.Latitude > 66)& (df2017Summary.Longitude < -156.7)]))# & (df2017Summary.Latitude  < 71.85)]))\n",
    "print(np.mean(df2018Summary['Fish 38 kHz sA (m^2 nmi^-2)'][(df2018Summary.Latitude > 66)]))# & (df2018Summary.Latitude  < 71.85)]))\n",
    "print(np.mean(df2019Summary['Fish 38 kHz sA (m^2 nmi^-2)'][(df2019Summary.Latitude > 66)& (df2019Summary.Longitude < -156.7)]))# & (df2019Summary.Latitude  < 71.85)]))\n",
    "print('2017 is X times higher than the other years: ',1094.893582625908/70.31331154280329,\n",
    "                                                      1094.893582625908/220.85999182865228,\n",
    "                                                    1094.893582625908/178.53462277255952,\n",
    "                                                      1094.893582625908/239.6171650003879)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pollock 2017: 154806537348.73053\n",
      "Pollock 2019: 38171505144.54589\n",
      "Pollock multiplier: 4.055552348866441\n",
      "Acod 2017: 569196315248.7784\n",
      "Acod 2019: 98214793439.78123\n",
      "Acod multiplier: 5.795423431784456\n",
      "Capelin 2017: 13249830616.098997\n",
      "Capelin 2019: 2392035129.4074864\n"
     ]
    }
   ],
   "source": [
    "a = pd.concat([pd.read_csv('../data/catchData/2017_2019/mbaCatchResults_2017.csv'),pd.read_csv('../data/catchData/2017_2019/mbaCatchResults_2019.csv')])\n",
    "a.SPECIES_CODE.unique()\n",
    "#pollock\n",
    "print('Pollock 2017:',a[(a.SURVEY==201701)&(a.SPECIES_CODE==21744)&(a.END_LONGITUDE <-156.7)].NUMBERS.sum())\n",
    "print('Pollock 2019:',a[(a.SURVEY==201901)&(a.SPECIES_CODE==21744)&(a.END_LONGITUDE <-156.7)].NUMBERS.sum())\n",
    "print('Pollock multiplier:',a[(a.SURVEY==201701)&(a.SPECIES_CODE==21744)&(a.END_LONGITUDE <-156.7)].NUMBERS.sum()/a[(a.SURVEY==201901)&(a.SPECIES_CODE==21744)&(a.END_LONGITUDE <-156.7)].NUMBERS.sum())\n",
    "#arctic cod\n",
    "print('Acod 2017:',a[(a.SURVEY==201701)&(a.SPECIES_CODE==21725)&(a.END_LONGITUDE <-156.7)].NUMBERS.sum())\n",
    "print('Acod 2019:',a[(a.SURVEY==201901)&(a.SPECIES_CODE==21725)&(a.END_LONGITUDE <-156.7)].NUMBERS.sum())\n",
    "print('Acod multiplier:',a[(a.SURVEY==201701)&(a.SPECIES_CODE==21725)&(a.END_LONGITUDE <-156.7)].NUMBERS.sum()/a[(a.SURVEY==201901)&(a.SPECIES_CODE==21725)&(a.END_LONGITUDE <-156.7)].NUMBERS.sum())\n",
    "# capelin\n",
    "print('Capelin 2017:',a[(a.SURVEY==201701)&(a.SPECIES_CODE==23041)&(a.END_LONGITUDE <-156.7)].NUMBERS.sum())\n",
    "print('Capelin 2019:',a[(a.SURVEY==201901)&(a.SPECIES_CODE==23041)&(a.END_LONGITUDE <-156.7)].NUMBERS.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 Gadids: 0.9822197898022202\n",
      "2019 Gadids: 0.9689804753552319\n",
      "2017 Arctic cod: 0.7639586392368041\n",
      "2019 Arctic cod: 0.6867540257360107\n",
      "2017 Pollock: 0.2118389888864264\n",
      "2019 Pollock: 0.26698767700760423\n"
     ]
    }
   ],
   "source": [
    "specs = [21725, 21744, 21740, 21735, 21720]\n",
    "print('2017 Gadids:',a[(a.SURVEY==201701)&(a.SPECIES_CODE.isin(specs))].NUMBERS.sum()/a[(a.SURVEY==201701)].NUMBERS.sum())\n",
    "print('2019 Gadids:',a[(a.SURVEY==201901)&(a.SPECIES_CODE.isin(specs))].NUMBERS.sum()/a[(a.SURVEY==201901)].NUMBERS.sum())\n",
    "\n",
    "print('2017 Arctic cod:',a[(a.SURVEY==201701)&(a.SPECIES_CODE==21725)].NUMBERS.sum()/a[(a.SURVEY==201701)].NUMBERS.sum())\n",
    "print('2019 Arctic cod:',a[(a.SURVEY==201901)&(a.SPECIES_CODE==21725)].NUMBERS.sum()/a[(a.SURVEY==201901)].NUMBERS.sum())\n",
    "\n",
    "print('2017 Pollock:',a[(a.SURVEY==201701)&(a.SPECIES_CODE.isin([21740,21744]))].NUMBERS.sum()/a[(a.SURVEY==201701)].NUMBERS.sum())\n",
    "print('2019 Pollock:',a[(a.SURVEY==201901)&(a.SPECIES_CODE.isin([21740,21744]))].NUMBERS.sum()/a[(a.SURVEY==201901)].NUMBERS.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201701 21725 56.91963152487784\n",
      "201701 21744 15.480653734873053\n",
      "201701 23041 1.3249830616098996\n",
      "201701 21720 0.27442608997467366\n",
      "201701 21735 0.1599452913092083\n",
      "201701 21110 0.0\n",
      "201701 21740 0.0\n",
      "201901 21725 9.821479343978124\n",
      "201901 21744 3.817150514454589\n",
      "201901 23041 0.23920351294074865\n",
      "201901 21720 0.09356861611194861\n",
      "201901 21735 0.12436573130185131\n",
      "201901 21110 0.20430775095464515\n",
      "201901 21740 0.0009466986394714001\n"
     ]
    }
   ],
   "source": [
    "#Abundances by survey by species\n",
    "for surv in a.SURVEY.unique():\n",
    "    for spec in a.SPECIES_CODE.unique():\n",
    "        print(surv,spec,a[(a.SURVEY==surv)&(a.SPECIES_CODE==spec)&(a.END_LONGITUDE <-156.7)].NUMBERS.sum()/10**10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012 Acod: 3.5271698674469336\n",
      "2013 Acod: 3.55174105365163\n",
      "2017 Acod: 4.488351987754867\n",
      "2019 Acod: 4.741594895765868\n",
      "2012 pollock: 4.971642230484027\n",
      "2013 pollock: 6.828973597980371\n",
      "2017 pollock: 4.996195540349967\n",
      "2019 pollock: 5.286786657042657\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# lengths of arctic cod and pollock\n",
    "df2012 = pd.read_csv('../data/catchData/2012_2013/Arctic_EIS_AT_survey_Arctic_cod_by_length_alongtrack_fish_per_m_squared_2012_v3.csv')\n",
    "df2013 = pd.read_csv('../data/catchData/2012_2013/Arctic_EIS_AT_survey_Arctic_cod_by_length_alongtrack_fish_per_m_squared_2013_v3.csv')\n",
    "df2017 = pd.read_csv('../data/catchData/2017_2019/Arctic_EIS_201701_21725.csv')\n",
    "df2019 = pd.read_csv('../data/catchData/2017_2019/Arctic_EIS_201901_21725.csv')\n",
    "print('2012 Acod:',sum(df2012[df2012.Latitude >=65.9].sum()[3:]/sum(df2012[df2012.Latitude >=65.9].sum()[3:])*np.arange(2,31,1)))\n",
    "print('2013 Acod:',sum(df2013[df2013.Latitude >=65.9].sum()[3:]/sum(df2013[df2013.Latitude >=65.9].sum()[3:])*np.arange(2,31,1)))\n",
    "print('2017 Acod:',sum(df2017.sum()[3:23]/sum(df2017.sum()[3:23])*np.arange(1,19,1)))\n",
    "print('2019 Acod:',sum(df2019.sum()[3:23]/sum(df2019.sum()[3:23])*np.arange(1,21,1)))\n",
    "\n",
    "#pollock\n",
    "df2012 = pd.read_csv('../data/catchData/2012_2013/Arctic_EIS_AT_survey_pollock_by_length_alongtrack_fish_per_m_squared_2012_v3.csv')\n",
    "df2013 = pd.read_csv('../data/catchData/2012_2013/Arctic_EIS_AT_survey_pollock_by_length_alongtrack_fish_per_m_squared_2013_v3.csv')\n",
    "df2017 = pd.read_csv('../data/catchData/2017_2019/Arctic_EIS_201701_21744.csv')\n",
    "df2019 = pd.read_csv('../data/catchData/2017_2019/Arctic_EIS_201901_21744.csv')\n",
    "print('2012 pollock:',sum(df2012.sum()[3:23]/sum(df2012.sum()[3:23])*np.arange(2,22,1)))\n",
    "print('2013 pollock:',sum(df2013.sum()[3:23]/sum(df2013.sum()[3:23])*np.arange(2,22,1)))\n",
    "print('2017 pollock:',sum(df2017.sum()[3:23]/sum(df2017.sum()[3:23])*np.arange(1,19,1)))\n",
    "print('2019 pollock:',sum(df2019.sum()[3:23]/sum(df2019.sum()[3:23])*np.arange(1,21,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proportion of acoustic backscatter attributed to the key species\n",
    "```sql\n",
    "select distinct a.survey, c.event_id, a.interval,  a.layer,a.prc_nasc, c.species_code, c.sa_proportion\n",
    "from \n",
    "(select * from integration_results) a\n",
    "join\n",
    "(select * from interval_scaling_key_map) b\n",
    "on a.interval = b.interval\n",
    "and a.ship = b.ship\n",
    "and a.survey = b.survey\n",
    "join\n",
    "(select * from scaling_key_sa_by_species) c\n",
    "on b.event_id = c.event_id\n",
    "and b.ship = c.ship\n",
    "and b.survey = c.survey\n",
    "where a.ship = 175 and (a.survey=201701 or a.survey = 201901)\n",
    "order by a.interval, c.species_code, layer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SPECIES_CODE</th>\n",
       "      <th>NASC_SPECIES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>21744</td>\n",
       "      <td>1.947403e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>21725</td>\n",
       "      <td>1.412332e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>21735</td>\n",
       "      <td>1.220434e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>23808</td>\n",
       "      <td>7.497329e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>23041</td>\n",
       "      <td>7.328262e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>21720</td>\n",
       "      <td>5.314928e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>23801</td>\n",
       "      <td>3.049450e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>21334</td>\n",
       "      <td>8.739858e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>21368</td>\n",
       "      <td>5.523867e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>23800</td>\n",
       "      <td>5.090230e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SPECIES_CODE  NASC_SPECIES\n",
       "16         21744  1.947403e+06\n",
       "14         21725  1.412332e+06\n",
       "15         21735  1.220434e+05\n",
       "25         23808  7.497329e+04\n",
       "19         23041  7.328262e+04\n",
       "13         21720  5.314928e+04\n",
       "21         23801  3.049450e+04\n",
       "8          21334  8.739858e+03\n",
       "9          21368  5.523867e+03\n",
       "20         23800  5.090230e+03"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prop key spec:  0.9626238092227528\n",
      "Prop gadid:  0.9430729556386999\n",
      "Prop jelly:  0.0016100818643171675\n",
      "Prop capelin/herring:  0.019550853584052928\n",
      "2019\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SPECIES_CODE</th>\n",
       "      <th>NASC_SPECIES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21744</td>\n",
       "      <td>398684.476779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>21725</td>\n",
       "      <td>172685.301463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>21735</td>\n",
       "      <td>48090.815494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>21110</td>\n",
       "      <td>27081.560506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>23807</td>\n",
       "      <td>16358.548515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>23041</td>\n",
       "      <td>13047.467517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>21720</td>\n",
       "      <td>9236.134823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>40504</td>\n",
       "      <td>6434.550876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>23800</td>\n",
       "      <td>5361.257048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21740</td>\n",
       "      <td>4878.295671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SPECIES_CODE   NASC_SPECIES\n",
       "21         21744  398684.476779\n",
       "18         21725  172685.301463\n",
       "19         21735   48090.815494\n",
       "9          21110   27081.560506\n",
       "31         23807   16358.548515\n",
       "27         23041   13047.467517\n",
       "17         21720    9236.134823\n",
       "33         40504    6434.550876\n",
       "29         23800    5361.257048\n",
       "20         21740    4878.295671"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prop key spec:  0.9390106354059371\n",
      "Prop gadid:  0.8830786813440535\n",
      "Prop jelly:  0.012811612584037527\n",
      "Prop capelin/herring:  0.055931954061883477\n"
     ]
    }
   ],
   "source": [
    "a = pd.read_csv('../data/acousticData/2017_2019/mbaSpeciesProp.csv')\n",
    "a['NASC_SPECIES'] = a.PRC_NASC*a.SA_PROPORTION\n",
    "\n",
    "c = a[a.SURVEY==201701]\n",
    "b = c.groupby(by=['SPECIES_CODE']).sum().reset_index()\n",
    "print('2017')\n",
    "display(b.sort_values(by='NASC_SPECIES',ascending=False)[['SPECIES_CODE','NASC_SPECIES']].head(10))\n",
    "specs = [21110, 23041, 21725, 21744, 21740, 21735, 21720]\n",
    "b[b.SPECIES_CODE.isin(specs)].NASC_SPECIES.sum()/b.NASC_SPECIES.sum()\n",
    "print('Prop key spec: ',b[b.SPECIES_CODE.isin(specs)].NASC_SPECIES.sum()/b.NASC_SPECIES.sum())\n",
    "specs = [21725, 21744, 21740, 21735, 21720]\n",
    "print('Prop gadid: ',b[b.SPECIES_CODE.isin(specs)].NASC_SPECIES.sum()/b.NASC_SPECIES.sum())\n",
    "print('Prop jelly: ',b[(b.SPECIES_CODE>40000)&(b.SPECIES_CODE<50000)].NASC_SPECIES.sum()/b.NASC_SPECIES.sum())\n",
    "print('Prop capelin/herring: ',b[b.SPECIES_CODE.isin([21110,23041])].NASC_SPECIES.sum()/b.NASC_SPECIES.sum())\n",
    "\n",
    "c = a[a.SURVEY==201901]\n",
    "print('2019')\n",
    "b = c.groupby(by=['SPECIES_CODE']).sum().reset_index()\n",
    "display(b.sort_values(by='NASC_SPECIES',ascending=False)[['SPECIES_CODE','NASC_SPECIES']].head(10))\n",
    "specs = [21110, 23041, 21725, 21744, 21740, 21735, 21720]\n",
    "print('Prop key spec: ',b[b.SPECIES_CODE.isin(specs)].NASC_SPECIES.sum()/b.NASC_SPECIES.sum())\n",
    "specs = [21725, 21744, 21740, 21735, 21720]\n",
    "print('Prop gadid: ',b[b.SPECIES_CODE.isin(specs)].NASC_SPECIES.sum()/b.NASC_SPECIES.sum())\n",
    "print('Prop jelly: ',b[(b.SPECIES_CODE>40000)&(b.SPECIES_CODE<50000)].NASC_SPECIES.sum()/b.NASC_SPECIES.sum())\n",
    "print('Prop capelin/herring: ',b[b.SPECIES_CODE.isin([21110,23041])].NASC_SPECIES.sum()/b.NASC_SPECIES.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total number of pollock found in catch north of the strait in 2012 and 2013 in the catch\n",
    "```sql \n",
    "select b.eq_latitude, a.*\n",
    "from\n",
    "(select * from v_aeis_catch_summary_v2 where survey=2012001 and (species_code = 21740 or species_code = 21744)) a\n",
    "join\n",
    "(select * from v_event_data) b\n",
    "on a.ship = b.ship and a.survey = b.survey and a.clams_event_number = b.event_id\n",
    "order by eq_latitude\n",
    "```\n",
    "\n",
    "28 in 2012, 2 in 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStat = pd.read_csv('catchAnalysis/analysisFiles/dfStat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.577283333333334 -0.7174166666666667\n",
      "34.73803333333333 31.094583333333336\n"
     ]
    }
   ],
   "source": [
    "#dfStat[dfStat.Year == 2017].T_surf.mean()\n",
    "print(dfStat[dfStat.Year == 2017].T_bot.max(),dfStat[dfStat.Year == 2017].T_bot.min())\n",
    "print(dfStat[dfStat.Year == 2017].S_bot.max(),dfStat[dfStat.Year == 2017].S_bot.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Genetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 genetics done:  894\n",
      "2017 all specimen:  2244\n",
      "2019 genetics done:  3155\n",
      "2019 all specimen:  5654\n"
     ]
    }
   ],
   "source": [
    "# Up to date\n",
    "a = pd.concat([pd.read_csv('C:/Users/robert.levine/Work/repositories/ArcticEISII/catchProcessing/code/specimenCorrections/geneticID/data/gadidsUpdatedLen2017.csv'),\n",
    "               pd.read_csv('C:/Users/robert.levine/Work/repositories/ArcticEISII/catchProcessing/code/specimenCorrections/geneticID/data/gadidsUpdatedLen2019.csv')])\n",
    "a = a[(a.GEAR=='Marinovich')&(a.PARTITION=='Codend')]\n",
    "print('2017 genetics done: ',len(a[(a.SURVEY==201701) & ((a.SPECIES_ID_METHOD=='genetics_changed')|(a.SPECIES_ID_METHOD=='genetics_confirmed'))]))\n",
    "print('2017 all specimen: ',len(a[(a.SURVEY==201701)]))\n",
    "\n",
    "print('2019 genetics done: ',len(a[(a.SURVEY==201901) & ((a.SPECIES_ID_METHOD=='genetics_changed')|(a.SPECIES_ID_METHOD=='genetics_confirmed'))]))\n",
    "print('2019 all specimen: ',len(a[(a.SURVEY==201901)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql select count(*) from v_aeis_specimen\n",
    "where survey = 201701 and\n",
    "gear = 'Marinovich'\n",
    "and net_partition = 'Codend'\n",
    "and species_code < 40000\n",
    "\n",
    "3392\n",
    "\n",
    "and species_code > 40000\n",
    "\n",
    "1211\n",
    "\n",
    "select count(*) from v_aeis_specimen\n",
    "where survey = 201701 and\n",
    "gear = 'Marinovich'\n",
    "and net_partition = 'Codend'\n",
    "and (species_code > 21700\n",
    "and species_code < 21800\n",
    "or species_code = 1202)\n",
    "order by species_code\n",
    "\n",
    "2244\n",
    "\n",
    "select count(*) from v_aeis_specimen\n",
    "where survey = 201701 and\n",
    "gear = 'Marinovich'\n",
    "and net_partition = 'Codend'\n",
    "and (species_id_method = 'genetics_confirmed'\n",
    "or species_id_method = 'genetics_changed')\n",
    "\n",
    "894\n",
    "\n",
    "select count(*) from v_aeis_specimen\n",
    "where survey = 201701 and\n",
    "gear = 'Marinovich'\n",
    "and net_partition = 'Codend'\n",
    "and species_id_method = 'genetics_model_assigned'\n",
    "\n",
    "1350\n",
    "\n",
    "select count(*) from v_aeis_specimen\n",
    "where survey = 201701 and\n",
    "gear = 'Marinovich'\n",
    "and net_partition = 'Codend'\n",
    "and species_id_method = 'field_id'\n",
    "\n",
    "0\n",
    "\n",
    "\n",
    "select count(*) from v_aeis_specimen\n",
    "where survey = 201901 and\n",
    "gear = 'Marinovich'\n",
    "and net_partition = 'Codend'\n",
    "and species_code < 40000\n",
    "\n",
    "9124\n",
    "\n",
    "and species_code > 40000\n",
    "\n",
    "751\n",
    "\n",
    "select count(*) from v_aeis_specimen\n",
    "where survey = 201901 and\n",
    "gear = 'Marinovich'\n",
    "and net_partition = 'Codend'\n",
    "and (species_code > 21700\n",
    "and species_code < 21800\n",
    "or species_code = 1202)\n",
    "order by species_code\n",
    "\n",
    "5676\n",
    "\n",
    "\n",
    "select count(*) from v_aeis_specimen\n",
    "where survey = 201901 and\n",
    "gear = 'Marinovich'\n",
    "and net_partition = 'Codend'\n",
    "and (species_id_method = 'genetics_confirmed'\n",
    "or species_id_method = 'genetics_changed')\n",
    "\n",
    "3155\n",
    "\n",
    "select count(*) from v_aeis_specimen\n",
    "where survey = 201901 and\n",
    "gear = 'Marinovich'\n",
    "and species_id_method = 'genetics_model_assigned'\n",
    "\n",
    "2488\n",
    "\n",
    "select count(*) from v_aeis_specimen\n",
    "where survey = 201901 and\n",
    "gear = 'Marinovich'\n",
    "and net_partition = 'Codend'\n",
    "and species_id_method = 'field_id'\n",
    "\n",
    "11\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
