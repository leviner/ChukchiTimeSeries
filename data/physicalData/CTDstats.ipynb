{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import gsw\n",
    "from geopy.distance import distance\n",
    "import numpy as np\n",
    "import seawater as sw\n",
    "\n",
    "def zmld_boyer(s, t, p):\n",
    "    \"\"\"\n",
    "    https://github.com/pyoceans/python-oceans/blob/master/oceans/sw_extras/sw_extras.py\n",
    "    Computes mixed layer depth, based on de Boyer Montégut et al., 2004.\n",
    "    Parameters\n",
    "    ----------\n",
    "    s : array_like\n",
    "        salinity [psu (PSS-78)]\n",
    "    t : array_like\n",
    "        temperature [℃ (ITS-90)]\n",
    "    p : array_like\n",
    "        pressure [db].\n",
    "    Notes\n",
    "    -----\n",
    "    Based on density with fixed threshold criteria\n",
    "    de Boyer Montégut et al., 2004. Mixed layer depth over the global ocean:\n",
    "        An examination of profile data and a profile-based climatology.\n",
    "        doi:10.1029/2004JC002378\n",
    "    dataset for test and more explanation can be found at:\n",
    "    http://www.ifremer.fr/cerweb/deboyer/mld/Surface_Mixed_Layer_Depth.php\n",
    "    Codes based on : http://mixedlayer.ucsd.edu/\n",
    "    \"\"\"\n",
    "    m = len(np.nonzero(~np.isnan(s))[0])\n",
    "\n",
    "    if m <= 1:\n",
    "        mldepthdens_mldindex = 0\n",
    "        mldepthptemp_mldindex = 0\n",
    "        return mldepthdens_mldindex, mldepthptemp_mldindex\n",
    "    else:\n",
    "        # starti = min(find((pres-10).^2==min((pres-10).^2)));\n",
    "        starti = np.min(\n",
    "            np.where(((p - 10.0) ** 2 == np.min((p - 10.0) ** 2)))[0]\n",
    "        )\n",
    "        starti = 0\n",
    "        pres = p[starti:m]\n",
    "        sal = s[starti:m]\n",
    "        temp = t[starti:m]\n",
    "\n",
    "        pden = sw.dens0(sal, temp) - 1000\n",
    "\n",
    "        mldepthdens_mldindex = m - 1\n",
    "        for i, pp in enumerate(pden):\n",
    "            if np.abs(pden[starti] - pp) > 0.03:\n",
    "                mldepthdens_mldindex = i\n",
    "                break\n",
    "\n",
    "        # Interpolate to exactly match the potential density threshold.\n",
    "        presseg = [pres[mldepthdens_mldindex - 1], pres[mldepthdens_mldindex]]\n",
    "        pdenseg = [\n",
    "            pden[starti] - pden[mldepthdens_mldindex - 1],\n",
    "            pden[starti] - pden[mldepthdens_mldindex],\n",
    "        ]\n",
    "        P = np.polyfit(presseg, pdenseg, 1)\n",
    "        presinterp = np.linspace(presseg[0], presseg[1], 3)\n",
    "        pdenthreshold = np.polyval(P, presinterp)\n",
    "\n",
    "        # The potential density threshold MLD value:\n",
    "        ix = np.max(np.where(np.abs(pdenthreshold) < 0.03)[0])\n",
    "        mldepthdens_mldindex = presinterp[ix]\n",
    "\n",
    "        # Search for the first level that exceeds the temperature threshold.\n",
    "        mldepthptmp_mldindex = m - 1\n",
    "        for i, tt in enumerate(temp):\n",
    "            if np.abs(temp[starti] - tt) > 0.2:\n",
    "                mldepthptmp_mldindex = i\n",
    "                break\n",
    "\n",
    "        # Interpolate to exactly match the temperature threshold.\n",
    "        presseg = [pres[mldepthptmp_mldindex - 1], pres[mldepthptmp_mldindex]]\n",
    "        tempseg = [\n",
    "            temp[starti] - temp[mldepthptmp_mldindex - 1],\n",
    "            temp[starti] - temp[mldepthptmp_mldindex],\n",
    "        ]\n",
    "        P = np.polyfit(presseg, tempseg, 1)\n",
    "        presinterp = np.linspace(presseg[0], presseg[1], 3)\n",
    "        tempthreshold = np.polyval(P, presinterp)\n",
    "\n",
    "        # The temperature threshold MLD value:\n",
    "        ix = np.max(np.where(np.abs(tempthreshold) < 0.2)[0])\n",
    "        mldepthptemp_mldindex = presinterp[ix]\n",
    "\n",
    "        return mldepthdens_mldindex, mldepthptemp_mldindex\n",
    "    \n",
    "dfCTD = pd.concat([pd.read_csv(file) for file in glob('CTD_os*.csv')])\n",
    "dfCTD = dfCTD[dfCTD['profile_id'].notna()]\n",
    "dfCTD = dfCTD.drop(labels=['S_42'],axis=1)\n",
    "dfCTD = dfCTD.astype({'pressure': float, 'latitude': float, 'longitude': float, 'S_41': float, 'T2_35': float,'T_28': float})\n",
    "dfCTD['time'] = pd.to_datetime(dfCTD.time)\n",
    "dfCTD['depth'] = -1*gsw.z_from_p(dfCTD.pressure, dfCTD.latitude)\n",
    "dfCTD = dfCTD.sort_values(by='time')\n",
    "dfCTD2017 = dfCTD[(dfCTD.time.dt.year == 2017) & (dfCTD.time > pd.to_datetime('8-01-2017').tz_localize ('UTC'))]\n",
    "dfCTD2019 = dfCTD[(dfCTD.time.dt.year == 2019) & (dfCTD.time > pd.to_datetime('8-01-2019').tz_localize ('UTC'))]\n",
    "dfCTD_clean = pd.concat([dfCTD2017,dfCTD2019])\n",
    "dfCTD_clean = dfCTD_clean.rename(columns={'S_41':'s','T_28':'t'}).drop(columns=['T2_35'])\n",
    "\n",
    "dfEvents = pd.read_csv('../catchData/2017_2019/AIERP_EventData.csv')\n",
    "dfEvents = dfEvents[['SURVEY','EVENT_ID','EQ_TIME','EQ_LONGITUDE','EQ_LATITUDE']]\n",
    "dfEvents['EQ_TIME'] = pd.to_datetime(dfEvents.EQ_TIME,format='%d-%b-%y %H.%M.%S.%f %p')\n",
    "dfEvents = dfEvents[dfEvents.SURVEY <1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctdStats(df):\n",
    "    dfStations = df[['profile_id','latitude','longitude','time']].drop_duplicates()\n",
    "    dists = []\n",
    "    for year in df.time.dt.year.unique():\n",
    "        dfCur = dfStations[dfStations.time.dt.year == year]\n",
    "        nPoints = len(dfCur.latitude)\n",
    "        for i in range(nPoints):\n",
    "            curDists = []\n",
    "            for ii in range(nPoints):\n",
    "                if ii == i:\n",
    "                    continue\n",
    "                curDists.append(distance((dfCur.latitude.values[i],dfCur.longitude.values[i]),(dfCur.latitude.values[ii],dfCur.longitude.values[ii])).nm)\n",
    "            dists.append(np.min(curDists))\n",
    "    dfStations['Radius'] = np.array(dists)/2\n",
    "    S_max, S_min, S_surf, S_bot, S_mean, T_min, T_max, T_surf, T_bot, T_mean,mld,Depth_max = [[] for i in range(12)]\n",
    "\n",
    "    for pid in df.profile_id.unique():\n",
    "        dfCur = df[(df.profile_id==pid)].sort_values('pressure')\n",
    "        S_max.append(dfCur.s.max())\n",
    "        S_min.append(dfCur.s.min())\n",
    "        S_surf.append(dfCur[dfCur.depth <= 5].s.mean())\n",
    "        S_bot.append(dfCur[dfCur.depth > dfCur.depth.max()-5].s.mean())\n",
    "        S_mean.append(dfCur.s.mean())\n",
    "        T_max.append(dfCur.t.max())\n",
    "        T_min.append(dfCur.t.min())\n",
    "        T_surf.append(dfCur[dfCur.depth <= 5].t.mean())\n",
    "        T_bot.append(dfCur[dfCur.depth > dfCur.depth.max()-5].t.mean())\n",
    "        T_mean.append(dfCur.t.mean())\n",
    "        Depth_max.append(dfCur.depth.max())\n",
    "        try:\n",
    "            mld1, mld2 = zmld_boyer(dfCur.dropna().s.values, dfCur.dropna().t.values, dfCur.dropna().pressure.values)\n",
    "        except:\n",
    "            print(pid)\n",
    "            mld2 = np.nan\n",
    "        mld.append(mld2)\n",
    "    dfStats = pd.DataFrame({'profile_id':df.profile_id.unique(),'Depth_max':Depth_max,'S_max':S_max, 'S_min':S_min, 'S_surf':S_surf, 'S_bot':S_bot,'S_mean':S_mean, 'T_min':T_min, 'T_max':T_max, 'T_surf':T_surf, 'T_bot':T_bot,'T_mean':T_mean,'MLD':mld})\n",
    "    return dfStations.merge(dfStats)\n",
    "\n",
    "def matchTrawl(dfEvents, dfCTDStats):\n",
    "    profileList,distList,timeList = [],[],[]\n",
    "    for year in [2017,2019]:\n",
    "        dfTrawls = dfEvents[dfEvents.SURVEY == (year*100)+1]\n",
    "        dfEnvCur = dfCTDStats[dfCTDStats.time.dt.year == year]\n",
    "        for i in range(len(dfTrawls)):\n",
    "            curTrawl = (dfTrawls.EQ_LATITUDE.values[i],dfTrawls.EQ_LONGITUDE.values[i])#(dfEnvCur.latitude.values[i],dfEnvCur.longitude.values[i])        \n",
    "            curDist = 99999\n",
    "            for ii in range(len(dfEnvCur)):\n",
    "                dist = distance(curTrawl,(dfEnvCur.latitude.values[ii],dfEnvCur.longitude.values[ii])).km\n",
    "                if dist < curDist:\n",
    "                    if abs((dfEnvCur.time.values[ii]-dfTrawls.EQ_TIME.values[i]).astype('timedelta64[D]')).astype('int') < 2:\n",
    "                        curDist = dist\n",
    "                        curProfile = dfEnvCur.profile_id.values[ii]\n",
    "                        curTime = dfEnvCur.time.values[ii]\n",
    "            profileList.append(curProfile)\n",
    "            distList.append(curDist)\n",
    "            timeList.append(curTime)\n",
    "    dfEvents['profile_id'] = profileList\n",
    "    dfEvents['CTD_DISTANCE'] = distList\n",
    "    dfTrawlCTD = pd.merge(dfEvents,dfCTDStats).drop(columns='Radius')\n",
    "    dfTrawlCTD = dfTrawlCTD.rename(columns={'profile_id':'CTD_PROFILE','latitude':'CTD_LATITUDE','longitude':'CTD_LONGITUDE','time':'CTD_TIME'})\n",
    "    return dfTrawlCTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfCTDStats = ctdStats(dfCTD_clean)\n",
    "dfTrawlCTD = matchTrawl(dfEvents,dfCTDStats)\n",
    "dfTrawlCTD.to_csv('Chukchi17_19_trawl_ctd.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional requests for Baker et al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add classifiers\n",
    "# 1 = meltwater\n",
    "# 2 = BCWW <32.4 sal\n",
    "# 3 = BCWW > 32.4\n",
    "# 4 ?\n",
    "# 5 = BCSW\n",
    "# 6 = ACW\n",
    "# 7 (new) AtlW\n",
    "\n",
    "def wmClass(temp,sal):\n",
    "    if (temp < 7) & (sal < 30):\n",
    "        wm = 1 # meltwater\n",
    "    elif (temp < 0) & (sal < 32.4) & (sal > 30):\n",
    "        wm = 2 # BCWW <32.4 sal\n",
    "    elif (temp < 0) & (sal > 32.4) & (sal < 33.5):\n",
    "        wm = 3 # BCWW > 32.4 sal\n",
    "    elif (temp > 0) & (temp < 7) &(sal > 30) & (sal < 33.5):\n",
    "        wm = 5 # BCSW\n",
    "    elif (temp > 7):\n",
    "        wm = 6 # ACW\n",
    "    elif  (temp < 1) &(sal > 33.5):\n",
    "        wm = 7 # AtlW\n",
    "    else:\n",
    "        wm = np.nan\n",
    "    return wm\n",
    "\n",
    "df = pd.read_csv('Chukchi17_19_ctd.csv')\n",
    "wmS,wmB = [],[]\n",
    "for index, row in df.iterrows():\n",
    "    wmS.append(wmClass(row.T_surf, row.S_surf))\n",
    "    wmB.append(wmClass(row.T_bot, row.S_bot))\n",
    "df['Water_surf'] = wmS\n",
    "df['Water_bot'] = wmB\n",
    "df.to_csv('Chukchi17_19_ctd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert netCDFS to csvs, for Baker\n",
    "import sys\n",
    "sys.path.insert(1, 'C:/Users/Robert/Documents/projects/rltools')\n",
    "#sys.path.insert(1, 'C:/Users/robert.levine/work/repositories/rltools')\n",
    "from ctdTools import readCtd\n",
    "files2017 = glob('ctdNC/*1701*')\n",
    "files2019 = glob('ctdNC/*1901*')\n",
    "[readCtd.nc2csv(file,2019) for file in files2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then stack all those csvs together by year\n",
    "files = sorted(glob('ctdNC/*2017*.csv'))\n",
    "bigDF = pd.concat([pd.read_csv(file) for file in files])\n",
    "bigDF.to_csv('data_IES_CTD_02&PAR_2017.csv')\n",
    "\n",
    "files = sorted(glob('ctdNC/*2019*.csv'))\n",
    "bigDF = pd.concat([pd.read_csv(file) for file in files])\n",
    "bigDF.to_csv('data_IES_CTD_02&PAR_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not converge 2012 105.0\n",
      "Could not converge 2012 101.0\n",
      "Could not converge 2012 130.0\n",
      "Could not converge 2013 29.0\n",
      "Could not converge 2013 169.0\n"
     ]
    }
   ],
   "source": [
    "# MLD for 2012 and 2013\n",
    "dfCasts = pd.concat([pd.read_csv('data_IES_CTD_02&PAR_2012.csv'), pd.read_csv('data_IES_CTD_02&PAR_2013.csv')])\n",
    "dfMaster = pd.read_csv('Chukchi12_13_ctd.csv')\n",
    "mld = []\n",
    "for index, row in dfMaster.iterrows():\n",
    "    dfCur = dfCasts[(dfCasts.Year == row.Year)&(dfCasts.StationNumber == row.Station)]\n",
    "    try:\n",
    "        mld1,mld2 = zmld_boyer(dfCur['PrimarySalinity PSU'].values, dfCur['PrimaryTemperature deg. C'].values,dfCur['Depth (m)'].values)\n",
    "    except:\n",
    "        print('Could not converge',row.Year, row.Station)\n",
    "        mld2 = np.nan\n",
    "    mld.append(mld2)\n",
    "dfMaster['MLD'] = mld\n",
    "dfMaster.to_csv('Chukchi12_13_ctd.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
