{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import gsw\n",
    "from geopy.distance import distance\n",
    "import numpy as np\n",
    "import seawater as sw\n",
    "\n",
    "def zmld_boyer(s, t, p):\n",
    "    \"\"\"\n",
    "    https://github.com/pyoceans/python-oceans/blob/master/oceans/sw_extras/sw_extras.py\n",
    "    Computes mixed layer depth, based on de Boyer Montégut et al., 2004.\n",
    "    Parameters\n",
    "    ----------\n",
    "    s : array_like\n",
    "        salinity [psu (PSS-78)]\n",
    "    t : array_like\n",
    "        temperature [℃ (ITS-90)]\n",
    "    p : array_like\n",
    "        pressure [db].\n",
    "    Notes\n",
    "    -----\n",
    "    Based on density with fixed threshold criteria\n",
    "    de Boyer Montégut et al., 2004. Mixed layer depth over the global ocean:\n",
    "        An examination of profile data and a profile-based climatology.\n",
    "        doi:10.1029/2004JC002378\n",
    "    dataset for test and more explanation can be found at:\n",
    "    http://www.ifremer.fr/cerweb/deboyer/mld/Surface_Mixed_Layer_Depth.php\n",
    "    Codes based on : http://mixedlayer.ucsd.edu/\n",
    "    \"\"\"\n",
    "    m = len(np.nonzero(~np.isnan(s))[0])\n",
    "\n",
    "    if m <= 1:\n",
    "        mldepthdens_mldindex = 0\n",
    "        mldepthptemp_mldindex = 0\n",
    "        return mldepthdens_mldindex, mldepthptemp_mldindex\n",
    "    else:\n",
    "        # starti = min(find((pres-10).^2==min((pres-10).^2)));\n",
    "        starti = np.min(\n",
    "            np.where(((p - 10.0) ** 2 == np.min((p - 10.0) ** 2)))[0]\n",
    "        )\n",
    "        starti = 0\n",
    "        pres = p[starti:m]\n",
    "        sal = s[starti:m]\n",
    "        temp = t[starti:m]\n",
    "\n",
    "        pden = sw.dens0(sal, temp) - 1000\n",
    "\n",
    "        mldepthdens_mldindex = m - 1\n",
    "        for i, pp in enumerate(pden):\n",
    "            if np.abs(pden[starti] - pp) > 0.03:\n",
    "                mldepthdens_mldindex = i\n",
    "                break\n",
    "\n",
    "        # Interpolate to exactly match the potential density threshold.\n",
    "        presseg = [pres[mldepthdens_mldindex - 1], pres[mldepthdens_mldindex]]\n",
    "        pdenseg = [\n",
    "            pden[starti] - pden[mldepthdens_mldindex - 1],\n",
    "            pden[starti] - pden[mldepthdens_mldindex],\n",
    "        ]\n",
    "        P = np.polyfit(presseg, pdenseg, 1)\n",
    "        presinterp = np.linspace(presseg[0], presseg[1], 3)\n",
    "        pdenthreshold = np.polyval(P, presinterp)\n",
    "\n",
    "        # The potential density threshold MLD value:\n",
    "        ix = np.max(np.where(np.abs(pdenthreshold) < 0.03)[0])\n",
    "        mldepthdens_mldindex = presinterp[ix]\n",
    "\n",
    "        # Search for the first level that exceeds the temperature threshold.\n",
    "        mldepthptmp_mldindex = m - 1\n",
    "        for i, tt in enumerate(temp):\n",
    "            if np.abs(temp[starti] - tt) > 0.2:\n",
    "                mldepthptmp_mldindex = i\n",
    "                break\n",
    "\n",
    "        # Interpolate to exactly match the temperature threshold.\n",
    "        presseg = [pres[mldepthptmp_mldindex - 1], pres[mldepthptmp_mldindex]]\n",
    "        tempseg = [\n",
    "            temp[starti] - temp[mldepthptmp_mldindex - 1],\n",
    "            temp[starti] - temp[mldepthptmp_mldindex],\n",
    "        ]\n",
    "        P = np.polyfit(presseg, tempseg, 1)\n",
    "        presinterp = np.linspace(presseg[0], presseg[1], 3)\n",
    "        tempthreshold = np.polyval(P, presinterp)\n",
    "\n",
    "        # The temperature threshold MLD value:\n",
    "        ix = np.max(np.where(np.abs(tempthreshold) < 0.2)[0])\n",
    "        mldepthptemp_mldindex = presinterp[ix]\n",
    "\n",
    "        return mldepthdens_mldindex, mldepthptemp_mldindex\n",
    "    \n",
    "dfCTD = pd.concat([pd.read_csv(file) for file in glob('CTD_os*.csv')])\n",
    "dfCTD = dfCTD[dfCTD['profile_id'].notna()]\n",
    "dfCTD = dfCTD.drop(labels=['S_42'],axis=1)\n",
    "dfCTD = dfCTD.astype({'pressure': float, 'latitude': float, 'longitude': float, 'S_41': float, 'T2_35': float,'T_28': float})\n",
    "dfCTD['time'] = pd.to_datetime(dfCTD.time)\n",
    "dfCTD['depth'] = -1*gsw.z_from_p(dfCTD.pressure, dfCTD.latitude)\n",
    "dfCTD = dfCTD.sort_values(by='time')\n",
    "dfCTD2017 = dfCTD[(dfCTD.time.dt.year == 2017) & (dfCTD.time > pd.to_datetime('8-01-2017').tz_localize ('UTC'))]\n",
    "dfCTD2019 = dfCTD[(dfCTD.time.dt.year == 2019) & (dfCTD.time > pd.to_datetime('8-01-2019').tz_localize ('UTC'))]\n",
    "dfCTD_clean = pd.concat([dfCTD2017,dfCTD2019])\n",
    "dfCTD_clean = dfCTD_clean.rename(columns={'S_41':'s','T_28':'t'}).drop(columns=['T2_35'])\n",
    "\n",
    "dfEvents = pd.read_csv('../catchData/2017_2019/AIERP_EventData.csv')\n",
    "dfEvents = dfEvents[['SURVEY','EVENT_ID','EQ_TIME','EQ_LONGITUDE','EQ_LATITUDE']]\n",
    "dfEvents['EQ_TIME'] = pd.to_datetime(dfEvents.EQ_TIME,format='%d-%b-%y %H.%M.%S.%f %p')\n",
    "dfEvents = dfEvents[dfEvents.SURVEY <1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctdStats(df):\n",
    "    dfStations = df[['profile_id','latitude','longitude','time']].drop_duplicates()\n",
    "    dists = []\n",
    "    for year in df.time.dt.year.unique():\n",
    "        dfCur = dfStations[dfStations.time.dt.year == year]\n",
    "        nPoints = len(dfCur.latitude)\n",
    "        for i in range(nPoints):\n",
    "            curDists = []\n",
    "            for ii in range(nPoints):\n",
    "                if ii == i:\n",
    "                    continue\n",
    "                curDists.append(distance((dfCur.latitude.values[i],dfCur.longitude.values[i]),(dfCur.latitude.values[ii],dfCur.longitude.values[ii])).nm)\n",
    "            dists.append(np.min(curDists))\n",
    "    dfStations['Radius'] = np.array(dists)/2\n",
    "    S_max, S_min, S_surf, S_bot, S_mean, T_min, T_max, T_surf, T_bot, T_mean,mld,Depth_max = [[] for i in range(12)]\n",
    "\n",
    "    for pid in df.profile_id.unique():\n",
    "        dfCur = df[(df.profile_id==pid)].sort_values('pressure')\n",
    "        S_max.append(dfCur.s.max())\n",
    "        S_min.append(dfCur.s.min())\n",
    "        S_surf.append(dfCur[dfCur.depth <= 5].s.mean())\n",
    "        S_bot.append(dfCur[dfCur.depth > dfCur.depth.max()-5].s.mean())\n",
    "        S_mean.append(dfCur.s.mean())\n",
    "        T_max.append(dfCur.t.max())\n",
    "        T_min.append(dfCur.t.min())\n",
    "        T_surf.append(dfCur[dfCur.depth <= 5].t.mean())\n",
    "        T_bot.append(dfCur[dfCur.depth > dfCur.depth.max()-5].t.mean())\n",
    "        T_mean.append(dfCur.t.mean())\n",
    "        Depth_max.append(dfCur.depth.max())\n",
    "        try:\n",
    "            mld1, mld2 = zmld_boyer(dfCur.dropna().s.values, dfCur.dropna().t.values, dfCur.dropna().pressure.values)\n",
    "        except:\n",
    "            print(pid)\n",
    "            mld2 = np.nan\n",
    "        mld.append(mld2)\n",
    "    dfStats = pd.DataFrame({'profile_id':df.profile_id.unique(),'Depth_max':Depth_max,'S_max':S_max, 'S_min':S_min, 'S_surf':S_surf, 'S_bot':S_bot,'S_mean':S_mean, 'T_min':T_min, 'T_max':T_max, 'T_surf':T_surf, 'T_bot':T_bot,'T_mean':T_mean,'MLD':mld})\n",
    "    return dfStations.merge(dfStats)\n",
    "\n",
    "def matchTrawl(dfEvents, dfCTDStats):\n",
    "    profileList,distList,timeList = [],[],[]\n",
    "    for year in [2017,2019]:\n",
    "        dfTrawls = dfEvents[dfEvents.SURVEY == (year*100)+1]\n",
    "        dfEnvCur = dfCTDStats[dfCTDStats.time.dt.year == year]\n",
    "        for i in range(len(dfTrawls)):\n",
    "            curTrawl = (dfTrawls.EQ_LATITUDE.values[i],dfTrawls.EQ_LONGITUDE.values[i])#(dfEnvCur.latitude.values[i],dfEnvCur.longitude.values[i])        \n",
    "            curDist = 99999\n",
    "            for ii in range(len(dfEnvCur)):\n",
    "                dist = distance(curTrawl,(dfEnvCur.latitude.values[ii],dfEnvCur.longitude.values[ii])).km\n",
    "                if dist < curDist:\n",
    "                    if abs((dfEnvCur.time.values[ii]-dfTrawls.EQ_TIME.values[i]).astype('timedelta64[D]')).astype('int') < 2:\n",
    "                        curDist = dist\n",
    "                        curProfile = dfEnvCur.profile_id.values[ii]\n",
    "                        curTime = dfEnvCur.time.values[ii]\n",
    "            profileList.append(curProfile)\n",
    "            distList.append(curDist)\n",
    "            timeList.append(curTime)\n",
    "    dfEvents['profile_id'] = profileList\n",
    "    dfEvents['CTD_DISTANCE'] = distList\n",
    "    dfTrawlCTD = pd.merge(dfEvents,dfCTDStats).drop(columns='Radius')\n",
    "    dfTrawlCTD = dfTrawlCTD.rename(columns={'profile_id':'CTD_PROFILE','latitude':'CTD_LATITUDE','longitude':'CTD_LONGITUDE','time':'CTD_TIME'})\n",
    "    return dfTrawlCTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfCTDStats = ctdStats(dfCTD_clean)\n",
    "dfTrawlCTD = matchTrawl(dfEvents,dfCTDStats)\n",
    "dfTrawlCTD.to_csv('Chukchi17_19_trawl_ctd.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
